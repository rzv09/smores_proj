{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6fdbf17630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../3OEC_current_flow.csv')\n",
    "\n",
    "df[\"O2_avg\"] = df[[\"O2_S1\", \"O2_S2\", \"O2_S3\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARE_np(prediction, truth):\n",
    "    epsilon = 1e-8  # Small value to prevent division by zero\n",
    "    return np.sum((np.abs(prediction - truth)) / np.abs(truth + epsilon)) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-11 14:00:00\n",
      "2017-07-11 14:00:00 2017-07-12 08:00:00\n",
      "518401\n",
      "2017-07-13 10:59:59.875000\n",
      "2017-07-13 10:59:59.875000 2017-07-14 06:00:00\n",
      "547202\n",
      "2017-07-15 10:00:00\n",
      "2017-07-15 10:00:00 2017-07-16 06:00:00\n",
      "576001\n",
      "2017-07-16 16:00:00\n",
      "2017-07-16 16:00:00 2017-07-17 06:00:00\n",
      "403201\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_time_11 = datetime(2017, 7, 11, 14, 0, 0)\n",
    "end_time_11 = datetime(2017, 7, 12, 8, 0, 0)\n",
    "\n",
    "start_time_13 = datetime(2017, 7, 13, 11, 0, 0)\n",
    "end_time_13 = datetime(2017, 7, 14, 6, 0, 0)\n",
    "\n",
    "start_time_15 = datetime(2017, 7, 15, 10, 0, 0)\n",
    "end_time_15 = datetime(2017, 7, 16, 6, 0, 0)\n",
    "\n",
    "start_time_16 = datetime(2017, 7, 16, 16, 0, 0)\n",
    "end_time_16 = datetime(2017, 7, 17, 6, 0, 0)\n",
    "\n",
    "deployments = {\n",
    "    \"3oec_2017_7_11_12\": {\"start\": start_time_11, \"end\": end_time_11},\n",
    "    \"3oec_2017_7_13_14\": {\"start\": start_time_13, \"end\": end_time_13},\n",
    "    \"3oec_2017_7_15_16\": {\"start\": start_time_15, \"end\": end_time_15},\n",
    "    \"3oec_2017_7_16_17\": {\"start\": start_time_16, \"end\": end_time_16}\n",
    "}\n",
    "\n",
    "date_ranges = []\n",
    "\n",
    "for deployment_name, deployment_info in deployments.items():\n",
    "    start_time = deployment_info[\"start\"]\n",
    "    end_time = deployment_info[\"end\"]\n",
    "    if deployment_name == \"3oec_2017_7_13_14\":\n",
    "        start_time -= timedelta(seconds=0.125)\n",
    "    print(start_time)\n",
    "\n",
    "    # Calculate total seconds and number of measurements\n",
    "    total_seconds = (end_time - start_time).total_seconds() + 0.125\n",
    "    num_measurements = int(total_seconds * 8)\n",
    "\n",
    "    # Create DatetimeIndex for the deployment\n",
    "    date_range = pd.date_range(start=start_time, periods=num_measurements, freq=f'{1000/8}ms')\n",
    "    print(date_range[0], date_range[-1])\n",
    "    print(len(date_range))\n",
    "    date_ranges.append(pd.Series(date_range))\n",
    "\n",
    "# Concatenate all DatetimeIndexes\n",
    "complete_index = pd.concat(date_ranges)\n",
    "\n",
    "# Set the complete index to your DataFrame\n",
    "df.index = complete_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop deployment column and resample\n",
    "df_resampled = df.drop(columns=['deployment', 't', 't_increase', 'Vx', 'Vy', 'Vz', 'P', 'O2_S1', 'O2_S2', 'O2_S3']).resample('1min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece = df_resampled[\"2017-07-11\":\"2017-07-12 06:00:00\"]\n",
    "second_piece = df_resampled[\"2017-07-13 12:00:00\":\"2017-07-14 06:00:00\"]\n",
    "third_piece = df_resampled[\"2017-07-15 12:00:00\":\"2017-07-16 6:00:00\"]\n",
    "fourth_piece = df_resampled[\"2017-07-16 16:00:00\":\"2017-07-17\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences and their corresponding target sequences from the input data.\n",
    "    The target sequence is half the size of the training sequence.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data.\n",
    "        seq_length (int): The length of each training sequence.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of input sequences.\n",
    "        np.ndarray: Array of target sequences (half the length of input sequences).\n",
    "    \"\"\"\n",
    "    target_length = seq_length // 3  # Target is 1/3 the size of the training sequence\n",
    "\n",
    "    if len(data) < seq_length + target_length:\n",
    "        raise ValueError(\"Data length must be at least seq_length + target_length.\")\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length - target_length + 1):\n",
    "        sequences.append(data[i:i+seq_length])                        # Input sequence\n",
    "        targets.append(data[i+seq_length:i+seq_length+target_length]) # Target sequence\n",
    "\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data piece 1\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(first_piece)\n",
    "train_df1 =first_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean1 = train_df1.mean()\n",
    "train_std1 = train_df1.std()\n",
    "\n",
    "train_df1 = (train_df1 - train_mean1) / train_std1\n",
    "\n",
    "# make sequences\n",
    "train_seq1, train_labels1 = create_sequences(train_df1.values, 90)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor1 = torch.FloatTensor(train_seq1).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor1 = torch.FloatTensor(train_labels1).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 2\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(second_piece)\n",
    "train_df2 =second_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean2 = train_df2.mean()\n",
    "train_std2 = train_df2.std()\n",
    "\n",
    "train_df2 = (train_df2 - train_mean2) / train_std2\n",
    "\n",
    "\n",
    "# make sequences\n",
    "train_seq2, train_labels2 = create_sequences(train_df2.values, 90)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor2 = torch.FloatTensor(train_seq2).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor2 = torch.FloatTensor(train_labels2).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 3\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(third_piece)\n",
    "train_df3 = third_piece\n",
    "\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean3 = train_df3.mean()\n",
    "train_std3 = train_df3.std()\n",
    "\n",
    "train_df3 = (train_df3 - train_mean3) / train_std3\n",
    "\n",
    "# make sequences\n",
    "train_seq3, train_labels3 = create_sequences(train_df3.values, 90)\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor3 = torch.FloatTensor(train_seq3).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor3 = torch.FloatTensor(train_labels3).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 4\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(fourth_piece)\n",
    "train_df4 = fourth_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean4 = train_df4.mean()\n",
    "train_std4 = train_df4.std()\n",
    "\n",
    "train_df4 = (train_df4 - train_mean4) / train_std4\n",
    "\n",
    "# make sequences\n",
    "train_seq4, train_labels4 = create_sequences(train_df4.values, 90)\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor4 = torch.FloatTensor(train_seq4).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor4 = torch.FloatTensor(train_labels4).to(device=device) # (batch, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test_1 = (first_piece[:] - first_piece.mean())/first_piece.std()\n",
    "test_seq_1, test_labels_1 = create_sequences(normalized_test_1.values, 90)\n",
    "test_tensor_seq_1 = torch.FloatTensor(test_seq_1).to(device=device)\n",
    "test_tensor_labels_1 = torch.FloatTensor(test_labels_1).to(device=device)\n",
    "\n",
    "normalized_test_2 = (second_piece[:] - second_piece.mean())/second_piece.std()\n",
    "test_seq_2, test_labels_2 = create_sequences(normalized_test_2.values, 90)\n",
    "test_tensor_seq_2 = torch.FloatTensor(test_seq_2).to(device=device)\n",
    "test_tensor_labels_2 = torch.FloatTensor(test_labels_2).to(device=device)\n",
    "\n",
    "normalized_test_3 = (third_piece[:] - third_piece.mean())/third_piece.std()\n",
    "test_seq_3, test_labels_3 = create_sequences(normalized_test_3.values, 90)\n",
    "test_tensor_seq_3 = torch.FloatTensor(test_seq_3).to(device='cuda')\n",
    "test_tensor_labels_3 = torch.FloatTensor(test_labels_3).to(device='cuda')\n",
    "\n",
    "normalized_test_4 = (fourth_piece[:] - fourth_piece.mean())/fourth_piece.std()\n",
    "test_seq_4, test_labels_4 = create_sequences(normalized_test_4.values, 90)\n",
    "test_tensor_seq_4 = torch.FloatTensor(test_seq_4).to(device='cuda')\n",
    "test_tensor_labels_4 = torch.FloatTensor(test_labels_4).to(device='cuda')\n",
    "\n",
    "test_labels_1_un = test_labels_1 * first_piece.std().values + first_piece.mean().values\n",
    "test_labels_2_un = test_labels_2 * second_piece.std().values + second_piece.mean().values\n",
    "test_labels_3_un = test_labels_3 * third_piece.std().values + third_piece.mean().values\n",
    "test_labels_4_un = test_labels_4 * fourth_piece.std().values + fourth_piece.mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 30)\n",
    "        # self.fc2 = nn.Linear(30, output_dim)\n",
    "        # self.do = nn.Dropout()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)  # Initial hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)  # Initial cell state\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out = self.do(out)\n",
    "        # print(out.shape)\n",
    "        out = self.fc1(out[:,-1, :]) # take the last hidden state because we assume it encodes all the information about the sequence\n",
    "        # out = self.fc2(out[, :])  # Take the last 30 time step output\n",
    "        return out.unsqueeze(2)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate model\n",
    "model12 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model12 = model12.to(device)  # Use \"cuda\" if you have a GPU\n",
    "criterion12 = nn.SmoothL1Loss()\n",
    "optimizer12 = optim.Adam(model12.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.47836756706237793\n",
      "Epoch 10, Train Loss: 0.4753947854042053\n",
      "Epoch 20, Train Loss: 0.4683681130409241\n",
      "Epoch 30, Train Loss: 0.43947476148605347\n",
      "Epoch 40, Train Loss: 0.28645992279052734\n",
      "Epoch 50, Train Loss: 0.14892318844795227\n",
      "Epoch 60, Train Loss: 0.13181905448436737\n",
      "Epoch 70, Train Loss: 0.1069473922252655\n",
      "Epoch 80, Train Loss: 0.09243323653936386\n",
      "Epoch 90, Train Loss: 0.08091265708208084\n",
      "Epoch 100, Train Loss: 0.07327289134263992\n",
      "Epoch 110, Train Loss: 0.06892047822475433\n",
      "Epoch 120, Train Loss: 0.06579449772834778\n",
      "Epoch 130, Train Loss: 0.06309078633785248\n",
      "Epoch 140, Train Loss: 0.060652755200862885\n",
      "Epoch 150, Train Loss: 0.05821968987584114\n",
      "Epoch 160, Train Loss: 0.055515944957733154\n",
      "Epoch 170, Train Loss: 0.052023012191057205\n",
      "Epoch 180, Train Loss: 0.047683704644441605\n",
      "Epoch 190, Train Loss: 0.0440942645072937\n",
      "Epoch 200, Train Loss: 0.03834317997097969\n",
      "Epoch 210, Train Loss: 0.035362765192985535\n",
      "Epoch 220, Train Loss: 0.03199510648846626\n",
      "Epoch 230, Train Loss: 0.028755487874150276\n",
      "Epoch 240, Train Loss: 0.026580674573779106\n",
      "Epoch 250, Train Loss: 0.022649580612778664\n",
      "Epoch 260, Train Loss: 0.020159462466835976\n",
      "Epoch 270, Train Loss: 0.018551915884017944\n",
      "Epoch 280, Train Loss: 0.017448747530579567\n",
      "Epoch 290, Train Loss: 0.01645508036017418\n",
      "Epoch 300, Train Loss: 0.01562737300992012\n",
      "Epoch 310, Train Loss: 0.01490991935133934\n",
      "Epoch 320, Train Loss: 0.014270035549998283\n",
      "Epoch 330, Train Loss: 0.013688121922314167\n",
      "Epoch 340, Train Loss: 0.013150002807378769\n",
      "Epoch 350, Train Loss: 0.012628688476979733\n",
      "Epoch 360, Train Loss: 0.012063996866345406\n",
      "Epoch 370, Train Loss: 0.0113605335354805\n",
      "Epoch 380, Train Loss: 0.011079315096139908\n",
      "Epoch 390, Train Loss: 0.010842031799256802\n",
      "Epoch 400, Train Loss: 0.010272842831909657\n",
      "Epoch 410, Train Loss: 0.009799114428460598\n",
      "Epoch 420, Train Loss: 0.009439638815820217\n",
      "Epoch 430, Train Loss: 0.009091964922845364\n",
      "Epoch 440, Train Loss: 0.008687125518918037\n",
      "Epoch 450, Train Loss: 0.00821771938353777\n",
      "Epoch 460, Train Loss: 0.007717052008956671\n",
      "Epoch 470, Train Loss: 0.007279819808900356\n",
      "Epoch 480, Train Loss: 0.006984560750424862\n",
      "Epoch 490, Train Loss: 0.006815411150455475\n",
      "Epoch 500, Train Loss: 0.006744213402271271\n",
      "Epoch 510, Train Loss: 0.006772160064429045\n",
      "Epoch 520, Train Loss: 0.006611607503145933\n",
      "Epoch 530, Train Loss: 0.006334381178021431\n",
      "Epoch 540, Train Loss: 0.00622321292757988\n",
      "Epoch 550, Train Loss: 0.006095279008150101\n",
      "Epoch 560, Train Loss: 0.00599690480157733\n",
      "Epoch 570, Train Loss: 0.005920226685702801\n",
      "Epoch 580, Train Loss: 0.005845549516379833\n",
      "Epoch 590, Train Loss: 0.005774437915533781\n",
      "Epoch 600, Train Loss: 0.005708212498575449\n",
      "Epoch 610, Train Loss: 0.005644390359520912\n",
      "Epoch 620, Train Loss: 0.005582669749855995\n",
      "Epoch 630, Train Loss: 0.005528134759515524\n",
      "Epoch 640, Train Loss: 0.005514089018106461\n",
      "Epoch 650, Train Loss: 0.0054421192035079\n",
      "Epoch 660, Train Loss: 0.005357993766665459\n",
      "Epoch 670, Train Loss: 0.005344965495169163\n",
      "Epoch 680, Train Loss: 0.0052459207363426685\n",
      "Epoch 690, Train Loss: 0.0054623400792479515\n",
      "Epoch 700, Train Loss: 0.005273014772683382\n",
      "Epoch 710, Train Loss: 0.009306471794843674\n",
      "Epoch 720, Train Loss: 0.006731689441949129\n",
      "Epoch 730, Train Loss: 0.005839775316417217\n",
      "Epoch 740, Train Loss: 0.005421356298029423\n",
      "Epoch 750, Train Loss: 0.005143325310200453\n",
      "Epoch 760, Train Loss: 0.004966177511960268\n",
      "Epoch 770, Train Loss: 0.004846345633268356\n",
      "Epoch 780, Train Loss: 0.004737071227282286\n",
      "Epoch 790, Train Loss: 0.004626637790352106\n",
      "Epoch 800, Train Loss: 0.004514441825449467\n",
      "Epoch 810, Train Loss: 0.004397641401737928\n",
      "Epoch 820, Train Loss: 0.00427670544013381\n",
      "Epoch 830, Train Loss: 0.004153956193476915\n",
      "Epoch 840, Train Loss: 0.0040333205834031105\n",
      "Epoch 850, Train Loss: 0.004690258298069239\n",
      "Epoch 860, Train Loss: 0.005935841705650091\n",
      "Epoch 870, Train Loss: 0.00543272215873003\n",
      "Epoch 880, Train Loss: 0.004391846247017384\n",
      "Epoch 890, Train Loss: 0.003786523127928376\n",
      "Epoch 900, Train Loss: 0.0035887390840798616\n",
      "Epoch 910, Train Loss: 0.0034755286760628223\n",
      "Epoch 920, Train Loss: 0.0034048263914883137\n",
      "Epoch 930, Train Loss: 0.0033283247612416744\n",
      "Epoch 940, Train Loss: 0.0032700877636671066\n",
      "Epoch 950, Train Loss: 0.0032190338242799044\n",
      "Epoch 960, Train Loss: 0.003173330333083868\n",
      "Epoch 970, Train Loss: 0.0031322832219302654\n",
      "Epoch 980, Train Loss: 0.0030948962084949017\n",
      "Epoch 990, Train Loss: 0.0030603972263634205\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model12.train()\n",
    "    \n",
    "    optimizer12.zero_grad()\n",
    "    y_pred = model12(train_seq_tensor1)\n",
    "    \n",
    "    loss = criterion12(y_pred, train_label_tensor1)\n",
    "    loss.backward()\n",
    "    optimizer12.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.04996645450592041\n",
      "Epoch 10, Train Loss: 0.025359220802783966\n",
      "Epoch 20, Train Loss: 0.02131466567516327\n",
      "Epoch 30, Train Loss: 0.019228655844926834\n",
      "Epoch 40, Train Loss: 0.020189249888062477\n",
      "Epoch 50, Train Loss: 0.019007541239261627\n",
      "Epoch 60, Train Loss: 0.017935512587428093\n",
      "Epoch 70, Train Loss: 0.01734199747443199\n",
      "Epoch 80, Train Loss: 0.017354262992739677\n",
      "Epoch 90, Train Loss: 0.01669156923890114\n",
      "Epoch 100, Train Loss: 0.016276737675070763\n",
      "Epoch 110, Train Loss: 0.01598689891397953\n",
      "Epoch 120, Train Loss: 0.015555349178612232\n",
      "Epoch 130, Train Loss: 0.015163114294409752\n",
      "Epoch 140, Train Loss: 0.014603639952838421\n",
      "Epoch 150, Train Loss: 0.014156438410282135\n",
      "Epoch 160, Train Loss: 0.013838090933859348\n",
      "Epoch 170, Train Loss: 0.013275429606437683\n",
      "Epoch 180, Train Loss: 0.012719825841486454\n",
      "Epoch 190, Train Loss: 0.012224774807691574\n",
      "Epoch 200, Train Loss: 0.01118222251534462\n",
      "Epoch 210, Train Loss: 0.009234367869794369\n",
      "Epoch 220, Train Loss: 0.010856512002646923\n",
      "Epoch 230, Train Loss: 0.009170792996883392\n",
      "Epoch 240, Train Loss: 0.009879987686872482\n",
      "Epoch 250, Train Loss: 0.00831898208707571\n",
      "Epoch 260, Train Loss: 0.007754260208457708\n",
      "Epoch 270, Train Loss: 0.0079544922336936\n",
      "Epoch 280, Train Loss: 0.007754611782729626\n",
      "Epoch 290, Train Loss: 0.007176602724939585\n",
      "Epoch 300, Train Loss: 0.007312643341720104\n",
      "Epoch 310, Train Loss: 0.008560247719287872\n",
      "Epoch 320, Train Loss: 0.007145533338189125\n",
      "Epoch 330, Train Loss: 0.006908010691404343\n",
      "Epoch 340, Train Loss: 0.0067633395083248615\n",
      "Epoch 350, Train Loss: 0.006703501101583242\n",
      "Epoch 360, Train Loss: 0.006594197358936071\n",
      "Epoch 370, Train Loss: 0.0065199038945138454\n",
      "Epoch 380, Train Loss: 0.0064881048165261745\n",
      "Epoch 390, Train Loss: 0.006433800328522921\n",
      "Epoch 400, Train Loss: 0.006374404300004244\n",
      "Epoch 410, Train Loss: 0.006394660100340843\n",
      "Epoch 420, Train Loss: 0.006349316798150539\n",
      "Epoch 430, Train Loss: 0.006398699712008238\n",
      "Epoch 440, Train Loss: 0.006269699428230524\n",
      "Epoch 450, Train Loss: 0.006221603602170944\n",
      "Epoch 460, Train Loss: 0.0064147720113396645\n",
      "Epoch 470, Train Loss: 0.0067870840430259705\n",
      "Epoch 480, Train Loss: 0.006369713693857193\n",
      "Epoch 490, Train Loss: 0.00622491305693984\n",
      "Epoch 500, Train Loss: 0.006109484005719423\n",
      "Epoch 510, Train Loss: 0.006057351361960173\n",
      "Epoch 520, Train Loss: 0.006018167827278376\n",
      "Epoch 530, Train Loss: 0.0059999930672347546\n",
      "Epoch 540, Train Loss: 0.00596111174672842\n",
      "Epoch 550, Train Loss: 0.00592432264238596\n",
      "Epoch 560, Train Loss: 0.005916580557823181\n",
      "Epoch 570, Train Loss: 0.007077058311551809\n",
      "Epoch 580, Train Loss: 0.006228741258382797\n",
      "Epoch 590, Train Loss: 0.006258692592382431\n",
      "Epoch 600, Train Loss: 0.006283534225076437\n",
      "Epoch 610, Train Loss: 0.006172539666295052\n",
      "Epoch 620, Train Loss: 0.006012538447976112\n",
      "Epoch 630, Train Loss: 0.005849454086273909\n",
      "Epoch 640, Train Loss: 0.006083224434405565\n",
      "Epoch 650, Train Loss: 0.006814136169850826\n",
      "Epoch 660, Train Loss: 0.006564257200807333\n",
      "Epoch 670, Train Loss: 0.006482408381998539\n",
      "Epoch 680, Train Loss: 0.006348747294396162\n",
      "Epoch 690, Train Loss: 0.006245408207178116\n",
      "Epoch 700, Train Loss: 0.006142950616776943\n",
      "Epoch 710, Train Loss: 0.006072560325264931\n",
      "Epoch 720, Train Loss: 0.006002415437251329\n",
      "Epoch 730, Train Loss: 0.005950275808572769\n",
      "Epoch 740, Train Loss: 0.005889156833291054\n",
      "Epoch 750, Train Loss: 0.005829644855111837\n",
      "Epoch 760, Train Loss: 0.005784487351775169\n",
      "Epoch 770, Train Loss: 0.005789972376078367\n",
      "Epoch 780, Train Loss: 0.0057480670511722565\n",
      "Epoch 790, Train Loss: 0.005640705116093159\n",
      "Epoch 800, Train Loss: 0.005690907593816519\n",
      "Epoch 810, Train Loss: 0.005488542374223471\n",
      "Epoch 820, Train Loss: 0.005778903607279062\n",
      "Epoch 830, Train Loss: 0.005387087818235159\n",
      "Epoch 840, Train Loss: 0.0052060820162296295\n",
      "Epoch 850, Train Loss: 0.005485463887453079\n",
      "Epoch 860, Train Loss: 0.005227634683251381\n",
      "Epoch 870, Train Loss: 0.0050056991167366505\n",
      "Epoch 880, Train Loss: 0.0053366003558039665\n",
      "Epoch 890, Train Loss: 0.004978297743946314\n",
      "Epoch 900, Train Loss: 0.0048767803236842155\n",
      "Epoch 910, Train Loss: 0.004759109113365412\n",
      "Epoch 920, Train Loss: 0.004646351560950279\n",
      "Epoch 930, Train Loss: 0.004607784561812878\n",
      "Epoch 940, Train Loss: 0.004569373559206724\n",
      "Epoch 950, Train Loss: 0.004553861450403929\n",
      "Epoch 960, Train Loss: 0.004526279401034117\n",
      "Epoch 970, Train Loss: 0.004552386235445738\n",
      "Epoch 980, Train Loss: 0.004632098134607077\n",
      "Epoch 990, Train Loss: 0.00452814344316721\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model12.train()\n",
    "    \n",
    "    optimizer12.zero_grad()\n",
    "    y_pred = model12(train_seq_tensor2)\n",
    "    \n",
    "    loss = criterion12(y_pred, train_label_tensor2)\n",
    "    loss.backward()\n",
    "    optimizer12.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model12.state_dict, 'model12_30step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 12 dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.006486666238077342\n"
     ]
    }
   ],
   "source": [
    "model12.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_1)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_1[i]\n",
    "    y_test_label = test_labels_1_un[i]\n",
    "    y_pred_test = model12(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * first_piece.std().values + first_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 12 dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.0017952547218344499\n"
     ]
    }
   ],
   "source": [
    "model12.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_2)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_2[i]\n",
    "    y_test_label = test_labels_2_un[i]\n",
    "    y_pred_test = model12(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * second_piece.std().values + second_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 12 dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.00652512794161521\n"
     ]
    }
   ],
   "source": [
    "model12.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_3)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_3[i]\n",
    "    y_test_label = test_labels_3_un[i]\n",
    "    y_pred_test = model12(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * third_piece.std().values + third_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 12 dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.008781973079321582\n"
     ]
    }
   ],
   "source": [
    "model12.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_4)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_4[i]\n",
    "    y_test_label = test_labels_4_un[i]\n",
    "    y_pred_test = model12(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * fourth_piece.std().values + fourth_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model123 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model123 = model123.to(device)  # Use \"cuda\" if you have a GPU\n",
    "criterion123 = nn.SmoothL1Loss()\n",
    "optimizer123 = optim.Adam(model123.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.47917822003364563\n",
      "Epoch 10, Train Loss: 0.476277619600296\n",
      "Epoch 20, Train Loss: 0.46947258710861206\n",
      "Epoch 30, Train Loss: 0.44069620966911316\n",
      "Epoch 40, Train Loss: 0.2930632531642914\n",
      "Epoch 50, Train Loss: 0.18702377378940582\n",
      "Epoch 60, Train Loss: 0.12578678131103516\n",
      "Epoch 70, Train Loss: 0.11633921414613724\n",
      "Epoch 80, Train Loss: 0.09690529108047485\n",
      "Epoch 90, Train Loss: 0.08370163291692734\n",
      "Epoch 100, Train Loss: 0.07496176660060883\n",
      "Epoch 110, Train Loss: 0.06966044753789902\n",
      "Epoch 120, Train Loss: 0.06630214303731918\n",
      "Epoch 130, Train Loss: 0.06353079527616501\n",
      "Epoch 140, Train Loss: 0.060938429087400436\n",
      "Epoch 150, Train Loss: 0.05834558233618736\n",
      "Epoch 160, Train Loss: 0.055464163422584534\n",
      "Epoch 170, Train Loss: 0.05168639123439789\n",
      "Epoch 180, Train Loss: 0.04687570035457611\n",
      "Epoch 190, Train Loss: 0.04193054884672165\n",
      "Epoch 200, Train Loss: 0.03688492625951767\n",
      "Epoch 210, Train Loss: 0.03275888413190842\n",
      "Epoch 220, Train Loss: 0.02838340774178505\n",
      "Epoch 230, Train Loss: 0.02402474172413349\n",
      "Epoch 240, Train Loss: 0.019979573786258698\n",
      "Epoch 250, Train Loss: 0.021431341767311096\n",
      "Epoch 260, Train Loss: 0.019491413608193398\n",
      "Epoch 270, Train Loss: 0.017478959634900093\n",
      "Epoch 280, Train Loss: 0.016792211681604385\n",
      "Epoch 290, Train Loss: 0.015998119488358498\n",
      "Epoch 300, Train Loss: 0.015430493280291557\n",
      "Epoch 310, Train Loss: 0.01493640523403883\n",
      "Epoch 320, Train Loss: 0.014500088058412075\n",
      "Epoch 330, Train Loss: 0.014096487313508987\n",
      "Epoch 340, Train Loss: 0.013724854215979576\n",
      "Epoch 350, Train Loss: 0.013375415466725826\n",
      "Epoch 360, Train Loss: 0.013037779368460178\n",
      "Epoch 370, Train Loss: 0.012710490263998508\n",
      "Epoch 380, Train Loss: 0.012413504533469677\n",
      "Epoch 390, Train Loss: 0.01215311698615551\n",
      "Epoch 400, Train Loss: 0.011908400803804398\n",
      "Epoch 410, Train Loss: 0.011672310531139374\n",
      "Epoch 420, Train Loss: 0.011440555565059185\n",
      "Epoch 430, Train Loss: 0.01121086347848177\n",
      "Epoch 440, Train Loss: 0.010985027998685837\n",
      "Epoch 450, Train Loss: 0.010768290609121323\n",
      "Epoch 460, Train Loss: 0.010572581551969051\n",
      "Epoch 470, Train Loss: 0.011066875420510769\n",
      "Epoch 480, Train Loss: 0.01065519917756319\n",
      "Epoch 490, Train Loss: 0.010329866781830788\n",
      "Epoch 500, Train Loss: 0.009933664463460445\n",
      "Epoch 510, Train Loss: 0.00966562144458294\n",
      "Epoch 520, Train Loss: 0.009368464350700378\n",
      "Epoch 530, Train Loss: 0.009820444509387016\n",
      "Epoch 540, Train Loss: 0.009428029879927635\n",
      "Epoch 550, Train Loss: 0.008892289362847805\n",
      "Epoch 560, Train Loss: 0.008276889100670815\n",
      "Epoch 570, Train Loss: 0.007581364829093218\n",
      "Epoch 580, Train Loss: 0.007226802874356508\n",
      "Epoch 590, Train Loss: 0.006986064370721579\n",
      "Epoch 600, Train Loss: 0.006785648874938488\n",
      "Epoch 610, Train Loss: 0.0061836340464651585\n",
      "Epoch 620, Train Loss: 0.0060593970119953156\n",
      "Epoch 630, Train Loss: 0.006673886440694332\n",
      "Epoch 640, Train Loss: 0.005830675829201937\n",
      "Epoch 650, Train Loss: 0.0056217689998447895\n",
      "Epoch 660, Train Loss: 0.0054186382330954075\n",
      "Epoch 670, Train Loss: 0.0052473838441073895\n",
      "Epoch 680, Train Loss: 0.0051066395826637745\n",
      "Epoch 690, Train Loss: 0.004978290293365717\n",
      "Epoch 700, Train Loss: 0.004849767312407494\n",
      "Epoch 710, Train Loss: 0.0047180065885186195\n",
      "Epoch 720, Train Loss: 0.004585533868521452\n",
      "Epoch 730, Train Loss: 0.005550538655370474\n",
      "Epoch 740, Train Loss: 0.0044779968447983265\n",
      "Epoch 750, Train Loss: 0.004272276535630226\n",
      "Epoch 760, Train Loss: 0.004187070764601231\n",
      "Epoch 770, Train Loss: 0.004444056656211615\n",
      "Epoch 780, Train Loss: 0.003986792638897896\n",
      "Epoch 790, Train Loss: 0.003979039844125509\n",
      "Epoch 800, Train Loss: 0.0037413635291159153\n",
      "Epoch 810, Train Loss: 0.0036955627147108316\n",
      "Epoch 820, Train Loss: 0.0037409367505460978\n",
      "Epoch 830, Train Loss: 0.0037805349566042423\n",
      "Epoch 840, Train Loss: 0.003783988766372204\n",
      "Epoch 850, Train Loss: 0.003491624491289258\n",
      "Epoch 860, Train Loss: 0.003507469082251191\n",
      "Epoch 870, Train Loss: 0.0034325947053730488\n",
      "Epoch 880, Train Loss: 0.0033305780962109566\n",
      "Epoch 890, Train Loss: 0.00335073284804821\n",
      "Epoch 900, Train Loss: 0.0034925704821944237\n",
      "Epoch 910, Train Loss: 0.00324775418266654\n",
      "Epoch 920, Train Loss: 0.0032183865550905466\n",
      "Epoch 930, Train Loss: 0.0031561413779854774\n",
      "Epoch 940, Train Loss: 0.003537967335432768\n",
      "Epoch 950, Train Loss: 0.003135774051770568\n",
      "Epoch 960, Train Loss: 0.0031470407266169786\n",
      "Epoch 970, Train Loss: 0.0030792320612818003\n",
      "Epoch 980, Train Loss: 0.003069241065531969\n",
      "Epoch 990, Train Loss: 0.0036259968765079975\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model123.train()\n",
    "    \n",
    "    optimizer123.zero_grad()\n",
    "    y_pred = model123(train_seq_tensor1)\n",
    "    \n",
    "    loss = criterion123(y_pred, train_label_tensor1)\n",
    "    loss.backward()\n",
    "    optimizer123.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.0578572079539299\n",
      "Epoch 10, Train Loss: 0.029935600236058235\n",
      "Epoch 20, Train Loss: 0.02190033160150051\n",
      "Epoch 30, Train Loss: 0.020444955676794052\n",
      "Epoch 40, Train Loss: 0.019224360585212708\n",
      "Epoch 50, Train Loss: 0.01844237931072712\n",
      "Epoch 60, Train Loss: 0.017817240208387375\n",
      "Epoch 70, Train Loss: 0.017333056777715683\n",
      "Epoch 80, Train Loss: 0.016913514584302902\n",
      "Epoch 90, Train Loss: 0.016534360125660896\n",
      "Epoch 100, Train Loss: 0.01616121269762516\n",
      "Epoch 110, Train Loss: 0.01577279344201088\n",
      "Epoch 120, Train Loss: 0.015351208858191967\n",
      "Epoch 130, Train Loss: 0.014847041107714176\n",
      "Epoch 140, Train Loss: 0.014075756072998047\n",
      "Epoch 150, Train Loss: 0.011811651289463043\n",
      "Epoch 160, Train Loss: 0.01129104197025299\n",
      "Epoch 170, Train Loss: 0.013670951128005981\n",
      "Epoch 180, Train Loss: 0.013289576396346092\n",
      "Epoch 190, Train Loss: 0.011477327905595303\n",
      "Epoch 200, Train Loss: 0.010716233402490616\n",
      "Epoch 210, Train Loss: 0.01044456847012043\n",
      "Epoch 220, Train Loss: 0.015613737516105175\n",
      "Epoch 230, Train Loss: 0.011330720037221909\n",
      "Epoch 240, Train Loss: 0.010329619981348515\n",
      "Epoch 250, Train Loss: 0.010205039754509926\n",
      "Epoch 260, Train Loss: 0.00997375138103962\n",
      "Epoch 270, Train Loss: 0.009819425642490387\n",
      "Epoch 280, Train Loss: 0.009616189636290073\n",
      "Epoch 290, Train Loss: 0.009304680861532688\n",
      "Epoch 300, Train Loss: 0.020022641867399216\n",
      "Epoch 310, Train Loss: 0.01347491703927517\n",
      "Epoch 320, Train Loss: 0.012971309013664722\n",
      "Epoch 330, Train Loss: 0.012395195662975311\n",
      "Epoch 340, Train Loss: 0.011883800849318504\n",
      "Epoch 350, Train Loss: 0.01157398708164692\n",
      "Epoch 360, Train Loss: 0.011256104335188866\n",
      "Epoch 370, Train Loss: 0.010963572189211845\n",
      "Epoch 380, Train Loss: 0.010617143474519253\n",
      "Epoch 390, Train Loss: 0.010157482698559761\n",
      "Epoch 400, Train Loss: 0.009550468996167183\n",
      "Epoch 410, Train Loss: 0.008751888759434223\n",
      "Epoch 420, Train Loss: 0.008331184275448322\n",
      "Epoch 430, Train Loss: 0.008041273802518845\n",
      "Epoch 440, Train Loss: 0.007803707383573055\n",
      "Epoch 450, Train Loss: 0.007619253825396299\n",
      "Epoch 460, Train Loss: 0.007469067350029945\n",
      "Epoch 470, Train Loss: 0.007335403468459845\n",
      "Epoch 480, Train Loss: 0.007209808565676212\n",
      "Epoch 490, Train Loss: 0.007088922429829836\n",
      "Epoch 500, Train Loss: 0.006970672868192196\n",
      "Epoch 510, Train Loss: 0.006853032857179642\n",
      "Epoch 520, Train Loss: 0.0067378743551671505\n",
      "Epoch 530, Train Loss: 0.006655257195234299\n",
      "Epoch 540, Train Loss: 0.006632938049733639\n",
      "Epoch 550, Train Loss: 0.006492540705949068\n",
      "Epoch 560, Train Loss: 0.00642632320523262\n",
      "Epoch 570, Train Loss: 0.0063536628149449825\n",
      "Epoch 580, Train Loss: 0.006288334261626005\n",
      "Epoch 590, Train Loss: 0.006219147238880396\n",
      "Epoch 600, Train Loss: 0.006150934845209122\n",
      "Epoch 610, Train Loss: 0.006079915910959244\n",
      "Epoch 620, Train Loss: 0.006004414055496454\n",
      "Epoch 630, Train Loss: 0.005921371281147003\n",
      "Epoch 640, Train Loss: 0.005824605002999306\n",
      "Epoch 650, Train Loss: 0.005728988908231258\n",
      "Epoch 660, Train Loss: 0.005604920908808708\n",
      "Epoch 670, Train Loss: 0.005438800435513258\n",
      "Epoch 680, Train Loss: 0.005303123965859413\n",
      "Epoch 690, Train Loss: 0.0051809027791023254\n",
      "Epoch 700, Train Loss: 0.0069419159553945065\n",
      "Epoch 710, Train Loss: 0.006228169891983271\n",
      "Epoch 720, Train Loss: 0.005782254971563816\n",
      "Epoch 730, Train Loss: 0.00551114184781909\n",
      "Epoch 740, Train Loss: 0.0053811087273061275\n",
      "Epoch 750, Train Loss: 0.005277576856315136\n",
      "Epoch 760, Train Loss: 0.0051882024854421616\n",
      "Epoch 770, Train Loss: 0.005107746925204992\n",
      "Epoch 780, Train Loss: 0.005035276524722576\n",
      "Epoch 790, Train Loss: 0.004967388231307268\n",
      "Epoch 800, Train Loss: 0.004906847141683102\n",
      "Epoch 810, Train Loss: 0.004850347992032766\n",
      "Epoch 820, Train Loss: 0.0047984798438847065\n",
      "Epoch 830, Train Loss: 0.0047492990270257\n",
      "Epoch 840, Train Loss: 0.004701648373156786\n",
      "Epoch 850, Train Loss: 0.004655212629586458\n",
      "Epoch 860, Train Loss: 0.004632220137864351\n",
      "Epoch 870, Train Loss: 0.004578378051519394\n",
      "Epoch 880, Train Loss: 0.004543905612081289\n",
      "Epoch 890, Train Loss: 0.00450507877394557\n",
      "Epoch 900, Train Loss: 0.004477820359170437\n",
      "Epoch 910, Train Loss: 0.004491834435611963\n",
      "Epoch 920, Train Loss: 0.0044281636364758015\n",
      "Epoch 930, Train Loss: 0.004402673337608576\n",
      "Epoch 940, Train Loss: 0.004379264544695616\n",
      "Epoch 950, Train Loss: 0.004357670433819294\n",
      "Epoch 960, Train Loss: 0.004336718935519457\n",
      "Epoch 970, Train Loss: 0.00432226387783885\n",
      "Epoch 980, Train Loss: 0.004298631101846695\n",
      "Epoch 990, Train Loss: 0.004285566974431276\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model123.train()\n",
    "    \n",
    "    optimizer123.zero_grad()\n",
    "    y_pred = model123(train_seq_tensor2)\n",
    "    \n",
    "    loss = criterion123(y_pred, train_label_tensor2)\n",
    "    loss.backward()\n",
    "    optimizer123.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.11098272353410721\n",
      "Epoch 10, Train Loss: 0.04813872277736664\n",
      "Epoch 20, Train Loss: 0.030873211100697517\n",
      "Epoch 30, Train Loss: 0.025025205686688423\n",
      "Epoch 40, Train Loss: 0.021152237430214882\n",
      "Epoch 50, Train Loss: 0.020658152177929878\n",
      "Epoch 60, Train Loss: 0.020544126629829407\n",
      "Epoch 70, Train Loss: 0.020221494138240814\n",
      "Epoch 80, Train Loss: 0.0170571431517601\n",
      "Epoch 90, Train Loss: 0.024117929860949516\n",
      "Epoch 100, Train Loss: 0.021127047017216682\n",
      "Epoch 110, Train Loss: 0.01582333631813526\n",
      "Epoch 120, Train Loss: 0.013477539643645287\n",
      "Epoch 130, Train Loss: 0.012381318025290966\n",
      "Epoch 140, Train Loss: 0.023574350401759148\n",
      "Epoch 150, Train Loss: 0.020463744178414345\n",
      "Epoch 160, Train Loss: 0.018213149160146713\n",
      "Epoch 170, Train Loss: 0.016685089096426964\n",
      "Epoch 180, Train Loss: 0.015592044219374657\n",
      "Epoch 190, Train Loss: 0.015022439882159233\n",
      "Epoch 200, Train Loss: 0.014589119702577591\n",
      "Epoch 210, Train Loss: 0.014246814884245396\n",
      "Epoch 220, Train Loss: 0.013963752426207066\n",
      "Epoch 230, Train Loss: 0.013108938932418823\n",
      "Epoch 240, Train Loss: 0.012527053244411945\n",
      "Epoch 250, Train Loss: 0.012720239348709583\n",
      "Epoch 260, Train Loss: 0.011326697655022144\n",
      "Epoch 270, Train Loss: 0.014086712151765823\n",
      "Epoch 280, Train Loss: 0.0118008553981781\n",
      "Epoch 290, Train Loss: 0.010567289777100086\n",
      "Epoch 300, Train Loss: 0.009827318601310253\n",
      "Epoch 310, Train Loss: 0.010091372765600681\n",
      "Epoch 320, Train Loss: 0.009361539036035538\n",
      "Epoch 330, Train Loss: 0.008955922909080982\n",
      "Epoch 340, Train Loss: 0.014352618716657162\n",
      "Epoch 350, Train Loss: 0.014661821536719799\n",
      "Epoch 360, Train Loss: 0.01084822602570057\n",
      "Epoch 370, Train Loss: 0.009769584983587265\n",
      "Epoch 380, Train Loss: 0.00872747041285038\n",
      "Epoch 390, Train Loss: 0.008283287286758423\n",
      "Epoch 400, Train Loss: 0.008044146932661533\n",
      "Epoch 410, Train Loss: 0.007690927945077419\n",
      "Epoch 420, Train Loss: 0.007502424996346235\n",
      "Epoch 430, Train Loss: 0.007341105490922928\n",
      "Epoch 440, Train Loss: 0.007228936534374952\n",
      "Epoch 450, Train Loss: 0.007143915630877018\n",
      "Epoch 460, Train Loss: 0.007063012570142746\n",
      "Epoch 470, Train Loss: 0.006981316022574902\n",
      "Epoch 480, Train Loss: 0.02008196711540222\n",
      "Epoch 490, Train Loss: 0.015971936285495758\n",
      "Epoch 500, Train Loss: 0.012797947973012924\n",
      "Epoch 510, Train Loss: 0.013418762013316154\n",
      "Epoch 520, Train Loss: 0.011135862208902836\n",
      "Epoch 530, Train Loss: 0.0112527534365654\n",
      "Epoch 540, Train Loss: 0.010177453979849815\n",
      "Epoch 550, Train Loss: 0.009114298969507217\n",
      "Epoch 560, Train Loss: 0.011377048678696156\n",
      "Epoch 570, Train Loss: 0.008780091069638729\n",
      "Epoch 580, Train Loss: 0.010204045102000237\n",
      "Epoch 590, Train Loss: 0.008449393324553967\n",
      "Epoch 600, Train Loss: 0.008259955793619156\n",
      "Epoch 610, Train Loss: 0.008066216483712196\n",
      "Epoch 620, Train Loss: 0.00789493415504694\n",
      "Epoch 630, Train Loss: 0.007722965441644192\n",
      "Epoch 640, Train Loss: 0.007613745518028736\n",
      "Epoch 650, Train Loss: 0.007515932898968458\n",
      "Epoch 660, Train Loss: 0.007433933671563864\n",
      "Epoch 670, Train Loss: 0.007378092966973782\n",
      "Epoch 680, Train Loss: 0.007282715290784836\n",
      "Epoch 690, Train Loss: 0.007186426315456629\n",
      "Epoch 700, Train Loss: 0.0070927562192082405\n",
      "Epoch 710, Train Loss: 0.006959985010325909\n",
      "Epoch 720, Train Loss: 0.006813026033341885\n",
      "Epoch 730, Train Loss: 0.006705588661134243\n",
      "Epoch 740, Train Loss: 0.006648076698184013\n",
      "Epoch 750, Train Loss: 0.006556257139891386\n",
      "Epoch 760, Train Loss: 0.006486892234534025\n",
      "Epoch 770, Train Loss: 0.00647776247933507\n",
      "Epoch 780, Train Loss: 0.006427145563066006\n",
      "Epoch 790, Train Loss: 0.006344618275761604\n",
      "Epoch 800, Train Loss: 0.0062957266345620155\n",
      "Epoch 810, Train Loss: 0.0062043811194598675\n",
      "Epoch 820, Train Loss: 0.006137631833553314\n",
      "Epoch 830, Train Loss: 0.006081116385757923\n",
      "Epoch 840, Train Loss: 0.0060379114001989365\n",
      "Epoch 850, Train Loss: 0.005984948482364416\n",
      "Epoch 860, Train Loss: 0.005944646429270506\n",
      "Epoch 870, Train Loss: 0.005933758337050676\n",
      "Epoch 880, Train Loss: 0.005918141454458237\n",
      "Epoch 890, Train Loss: 0.00580912921577692\n",
      "Epoch 900, Train Loss: 0.00575075251981616\n",
      "Epoch 910, Train Loss: 0.00571272661909461\n",
      "Epoch 920, Train Loss: 0.005643374752253294\n",
      "Epoch 930, Train Loss: 0.005621638149023056\n",
      "Epoch 940, Train Loss: 0.0060550556518137455\n",
      "Epoch 950, Train Loss: 0.007131374441087246\n",
      "Epoch 960, Train Loss: 0.012049540877342224\n",
      "Epoch 970, Train Loss: 0.009147388860583305\n",
      "Epoch 980, Train Loss: 0.007715221028774977\n",
      "Epoch 990, Train Loss: 0.011897135525941849\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model123.train()\n",
    "    \n",
    "    optimizer123.zero_grad()\n",
    "    y_pred = model123(train_seq_tensor3)\n",
    "    \n",
    "    loss = criterion123(y_pred, train_label_tensor3)\n",
    "    loss.backward()\n",
    "    optimizer123.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model123.state_dict, 'model123_30step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 123 dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.0081342174702184\n"
     ]
    }
   ],
   "source": [
    "model123.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_1)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_1[i]\n",
    "    y_test_label = test_labels_1_un[i]\n",
    "    y_pred_test = model123(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * first_piece.std().values + first_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 123 dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.007515270791341105\n"
     ]
    }
   ],
   "source": [
    "model123.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_2)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_2[i]\n",
    "    y_test_label = test_labels_2_un[i]\n",
    "    y_pred_test = model123(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * second_piece.std().values + second_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 123 dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.0016922676836385467\n"
     ]
    }
   ],
   "source": [
    "model123.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_3)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_3[i]\n",
    "    y_test_label = test_labels_3_un[i]\n",
    "    y_pred_test = model123(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * third_piece.std().values + third_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 123 dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.008531851870275827\n"
     ]
    }
   ],
   "source": [
    "model123.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_4)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_4[i]\n",
    "    y_test_label = test_labels_4_un[i]\n",
    "    y_pred_test = model123(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * fourth_piece.std().values + fourth_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model124 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model124 = model124.to(device)  # Use \"cuda\" if you have a GPU\n",
    "criterion124 = nn.SmoothL1Loss()\n",
    "optimizer124 = optim.Adam(model124.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.4786578118801117\n",
      "Epoch 10, Train Loss: 0.4757179915904999\n",
      "Epoch 20, Train Loss: 0.46889129281044006\n",
      "Epoch 30, Train Loss: 0.4396200478076935\n",
      "Epoch 40, Train Loss: 0.28651735186576843\n",
      "Epoch 50, Train Loss: 0.18400731682777405\n",
      "Epoch 60, Train Loss: 0.1330108493566513\n",
      "Epoch 70, Train Loss: 0.11879938840866089\n",
      "Epoch 80, Train Loss: 0.09723170101642609\n",
      "Epoch 90, Train Loss: 0.08358990401029587\n",
      "Epoch 100, Train Loss: 0.07411117851734161\n",
      "Epoch 110, Train Loss: 0.06837597489356995\n",
      "Epoch 120, Train Loss: 0.06493419408798218\n",
      "Epoch 130, Train Loss: 0.0621645525097847\n",
      "Epoch 140, Train Loss: 0.05963553860783577\n",
      "Epoch 150, Train Loss: 0.05717995762825012\n",
      "Epoch 160, Train Loss: 0.054595377296209335\n",
      "Epoch 170, Train Loss: 0.05159774050116539\n",
      "Epoch 180, Train Loss: 0.04755287989974022\n",
      "Epoch 190, Train Loss: 0.045041751116514206\n",
      "Epoch 200, Train Loss: 0.042288996279239655\n",
      "Epoch 210, Train Loss: 0.03778833523392677\n",
      "Epoch 220, Train Loss: 0.03335537761449814\n",
      "Epoch 230, Train Loss: 0.03149261325597763\n",
      "Epoch 240, Train Loss: 0.028092067688703537\n",
      "Epoch 250, Train Loss: 0.02595674805343151\n",
      "Epoch 260, Train Loss: 0.022721948102116585\n",
      "Epoch 270, Train Loss: 0.020277151837944984\n",
      "Epoch 280, Train Loss: 0.018735162913799286\n",
      "Epoch 290, Train Loss: 0.017500657588243484\n",
      "Epoch 300, Train Loss: 0.01663142815232277\n",
      "Epoch 310, Train Loss: 0.015898259356617928\n",
      "Epoch 320, Train Loss: 0.015274803154170513\n",
      "Epoch 330, Train Loss: 0.014719813130795956\n",
      "Epoch 340, Train Loss: 0.014212673529982567\n",
      "Epoch 350, Train Loss: 0.01374561246484518\n",
      "Epoch 360, Train Loss: 0.013320100493729115\n",
      "Epoch 370, Train Loss: 0.012931648641824722\n",
      "Epoch 380, Train Loss: 0.0125817209482193\n",
      "Epoch 390, Train Loss: 0.012266799807548523\n",
      "Epoch 400, Train Loss: 0.011982996016740799\n",
      "Epoch 410, Train Loss: 0.011735009029507637\n",
      "Epoch 420, Train Loss: 0.012987334281206131\n",
      "Epoch 430, Train Loss: 0.012164903804659843\n",
      "Epoch 440, Train Loss: 0.01126135140657425\n",
      "Epoch 450, Train Loss: 0.01118142157793045\n",
      "Epoch 460, Train Loss: 0.011717995628714561\n",
      "Epoch 470, Train Loss: 0.011196772567927837\n",
      "Epoch 480, Train Loss: 0.012507552281022072\n",
      "Epoch 490, Train Loss: 0.013600204139947891\n",
      "Epoch 500, Train Loss: 0.012062260881066322\n",
      "Epoch 510, Train Loss: 0.010744033381342888\n",
      "Epoch 520, Train Loss: 0.010200481861829758\n",
      "Epoch 530, Train Loss: 0.009811749681830406\n",
      "Epoch 540, Train Loss: 0.009477527812123299\n",
      "Epoch 550, Train Loss: 0.009136205539107323\n",
      "Epoch 560, Train Loss: 0.008768854662775993\n",
      "Epoch 570, Train Loss: 0.008359762839972973\n",
      "Epoch 580, Train Loss: 0.007935338653624058\n",
      "Epoch 590, Train Loss: 0.027761291712522507\n",
      "Epoch 600, Train Loss: 0.013098533265292645\n",
      "Epoch 610, Train Loss: 0.011450783349573612\n",
      "Epoch 620, Train Loss: 0.009357134811580181\n",
      "Epoch 630, Train Loss: 0.008212847635149956\n",
      "Epoch 640, Train Loss: 0.0074417139403522015\n",
      "Epoch 650, Train Loss: 0.007142635062336922\n",
      "Epoch 660, Train Loss: 0.006956125609576702\n",
      "Epoch 670, Train Loss: 0.006831970065832138\n",
      "Epoch 680, Train Loss: 0.006733076646924019\n",
      "Epoch 690, Train Loss: 0.006649692077189684\n",
      "Epoch 700, Train Loss: 0.006575734820216894\n",
      "Epoch 710, Train Loss: 0.006507072597742081\n",
      "Epoch 720, Train Loss: 0.006442127283662558\n",
      "Epoch 730, Train Loss: 0.006379251834005117\n",
      "Epoch 740, Train Loss: 0.00631894962862134\n",
      "Epoch 750, Train Loss: 0.006260054185986519\n",
      "Epoch 760, Train Loss: 0.006202445365488529\n",
      "Epoch 770, Train Loss: 0.0061449166387319565\n",
      "Epoch 780, Train Loss: 0.0060881925746798515\n",
      "Epoch 790, Train Loss: 0.006031534168869257\n",
      "Epoch 800, Train Loss: 0.0059748124331235886\n",
      "Epoch 810, Train Loss: 0.005918706767261028\n",
      "Epoch 820, Train Loss: 0.00586217176169157\n",
      "Epoch 830, Train Loss: 0.0058058397844433784\n",
      "Epoch 840, Train Loss: 0.00574984448030591\n",
      "Epoch 850, Train Loss: 0.005694581661373377\n",
      "Epoch 860, Train Loss: 0.005639669485390186\n",
      "Epoch 870, Train Loss: 0.005585487000644207\n",
      "Epoch 880, Train Loss: 0.005531622096896172\n",
      "Epoch 890, Train Loss: 0.005478498991578817\n",
      "Epoch 900, Train Loss: 0.005425611976534128\n",
      "Epoch 910, Train Loss: 0.005370929837226868\n",
      "Epoch 920, Train Loss: 0.005320108029991388\n",
      "Epoch 930, Train Loss: 0.005266366060823202\n",
      "Epoch 940, Train Loss: 0.005244395695626736\n",
      "Epoch 950, Train Loss: 0.007395537104457617\n",
      "Epoch 960, Train Loss: 0.0064599537290632725\n",
      "Epoch 970, Train Loss: 0.00596387404948473\n",
      "Epoch 980, Train Loss: 0.005702806171029806\n",
      "Epoch 990, Train Loss: 0.005448969081044197\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model124.train()\n",
    "    \n",
    "    optimizer124.zero_grad()\n",
    "    y_pred = model124(train_seq_tensor1)\n",
    "    \n",
    "    loss = criterion124(y_pred, train_label_tensor1)\n",
    "    loss.backward()\n",
    "    optimizer124.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.05277636647224426\n",
      "Epoch 10, Train Loss: 0.025545308366417885\n",
      "Epoch 20, Train Loss: 0.023558514192700386\n",
      "Epoch 30, Train Loss: 0.02070009708404541\n",
      "Epoch 40, Train Loss: 0.01965363882482052\n",
      "Epoch 50, Train Loss: 0.018774697557091713\n",
      "Epoch 60, Train Loss: 0.01826183684170246\n",
      "Epoch 70, Train Loss: 0.01784948632121086\n",
      "Epoch 80, Train Loss: 0.01750996522605419\n",
      "Epoch 90, Train Loss: 0.017197223380208015\n",
      "Epoch 100, Train Loss: 0.016890998929739\n",
      "Epoch 110, Train Loss: 0.016598952934145927\n",
      "Epoch 120, Train Loss: 0.01630241610109806\n",
      "Epoch 130, Train Loss: 0.016686322167515755\n",
      "Epoch 140, Train Loss: 0.0159809160977602\n",
      "Epoch 150, Train Loss: 0.015231838449835777\n",
      "Epoch 160, Train Loss: 0.014716466888785362\n",
      "Epoch 170, Train Loss: 0.013975403271615505\n",
      "Epoch 180, Train Loss: 0.01320638321340084\n",
      "Epoch 190, Train Loss: 0.012558561749756336\n",
      "Epoch 200, Train Loss: 0.01150307897478342\n",
      "Epoch 210, Train Loss: 0.009287677705287933\n",
      "Epoch 220, Train Loss: 0.008533104322850704\n",
      "Epoch 230, Train Loss: 0.007793975993990898\n",
      "Epoch 240, Train Loss: 0.007569568231701851\n",
      "Epoch 250, Train Loss: 0.007216280326247215\n",
      "Epoch 260, Train Loss: 0.0069589861668646336\n",
      "Epoch 270, Train Loss: 0.006821992341428995\n",
      "Epoch 280, Train Loss: 0.0075963023118674755\n",
      "Epoch 290, Train Loss: 0.006694520357996225\n",
      "Epoch 300, Train Loss: 0.00660288892686367\n",
      "Epoch 310, Train Loss: 0.0064633674919605255\n",
      "Epoch 320, Train Loss: 0.006337057799100876\n",
      "Epoch 330, Train Loss: 0.006246743258088827\n",
      "Epoch 340, Train Loss: 0.006171143148094416\n",
      "Epoch 350, Train Loss: 0.006104090716689825\n",
      "Epoch 360, Train Loss: 0.006044892594218254\n",
      "Epoch 370, Train Loss: 0.005988993216305971\n",
      "Epoch 380, Train Loss: 0.005937938112765551\n",
      "Epoch 390, Train Loss: 0.005901338066905737\n",
      "Epoch 400, Train Loss: 0.005849885754287243\n",
      "Epoch 410, Train Loss: 0.005830356851220131\n",
      "Epoch 420, Train Loss: 0.00576580036431551\n",
      "Epoch 430, Train Loss: 0.00572578702121973\n",
      "Epoch 440, Train Loss: 0.0056872921995818615\n",
      "Epoch 450, Train Loss: 0.005670131184160709\n",
      "Epoch 460, Train Loss: 0.005626631900668144\n",
      "Epoch 470, Train Loss: 0.00559797091409564\n",
      "Epoch 480, Train Loss: 0.005564567167311907\n",
      "Epoch 490, Train Loss: 0.005524130072444677\n",
      "Epoch 500, Train Loss: 0.005482091568410397\n",
      "Epoch 510, Train Loss: 0.005448044277727604\n",
      "Epoch 520, Train Loss: 0.005425841547548771\n",
      "Epoch 530, Train Loss: 0.005376946646720171\n",
      "Epoch 540, Train Loss: 0.005345990415662527\n",
      "Epoch 550, Train Loss: 0.0053612422198057175\n",
      "Epoch 560, Train Loss: 0.005265768151730299\n",
      "Epoch 570, Train Loss: 0.0052368794567883015\n",
      "Epoch 580, Train Loss: 0.005203675478696823\n",
      "Epoch 590, Train Loss: 0.0052160294726490974\n",
      "Epoch 600, Train Loss: 0.005089435260742903\n",
      "Epoch 610, Train Loss: 0.005030549596995115\n",
      "Epoch 620, Train Loss: 0.004964820109307766\n",
      "Epoch 630, Train Loss: 0.0067666443064808846\n",
      "Epoch 640, Train Loss: 0.005579560529440641\n",
      "Epoch 650, Train Loss: 0.005075179040431976\n",
      "Epoch 660, Train Loss: 0.0048949443735182285\n",
      "Epoch 670, Train Loss: 0.004752026405185461\n",
      "Epoch 680, Train Loss: 0.004621349275112152\n",
      "Epoch 690, Train Loss: 0.005270142573863268\n",
      "Epoch 700, Train Loss: 0.005014343652874231\n",
      "Epoch 710, Train Loss: 0.005341340787708759\n",
      "Epoch 720, Train Loss: 0.004674418363720179\n",
      "Epoch 730, Train Loss: 0.004244394134730101\n",
      "Epoch 740, Train Loss: 0.004061718471348286\n",
      "Epoch 750, Train Loss: 0.003968586213886738\n",
      "Epoch 760, Train Loss: 0.003897387068718672\n",
      "Epoch 770, Train Loss: 0.0038992264308035374\n",
      "Epoch 780, Train Loss: 0.005002348218113184\n",
      "Epoch 790, Train Loss: 0.0051709427498281\n",
      "Epoch 800, Train Loss: 0.005910390056669712\n",
      "Epoch 810, Train Loss: 0.005144975148141384\n",
      "Epoch 820, Train Loss: 0.004758947063237429\n",
      "Epoch 830, Train Loss: 0.0045870216563344\n",
      "Epoch 840, Train Loss: 0.004402911756187677\n",
      "Epoch 850, Train Loss: 0.00434003584086895\n",
      "Epoch 860, Train Loss: 0.00428462540730834\n",
      "Epoch 870, Train Loss: 0.0042397440411150455\n",
      "Epoch 880, Train Loss: 0.004195837303996086\n",
      "Epoch 890, Train Loss: 0.004150806926190853\n",
      "Epoch 900, Train Loss: 0.004100186750292778\n",
      "Epoch 910, Train Loss: 0.0040447604842484\n",
      "Epoch 920, Train Loss: 0.003983409143984318\n",
      "Epoch 930, Train Loss: 0.003917431924492121\n",
      "Epoch 940, Train Loss: 0.0038506356067955494\n",
      "Epoch 950, Train Loss: 0.003784493776038289\n",
      "Epoch 960, Train Loss: 0.003722097724676132\n",
      "Epoch 970, Train Loss: 0.0038850551936775446\n",
      "Epoch 980, Train Loss: 0.0036371368914842606\n",
      "Epoch 990, Train Loss: 0.0036135632544755936\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model124.train()\n",
    "    \n",
    "    optimizer124.zero_grad()\n",
    "    y_pred = model124(train_seq_tensor2)\n",
    "    \n",
    "    loss = criterion124(y_pred, train_label_tensor2)\n",
    "    loss.backward()\n",
    "    optimizer124.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.11766194552183151\n",
      "Epoch 10, Train Loss: 0.04128238186240196\n",
      "Epoch 20, Train Loss: 0.027846086770296097\n",
      "Epoch 30, Train Loss: 0.023181550204753876\n",
      "Epoch 40, Train Loss: 0.02023240737617016\n",
      "Epoch 50, Train Loss: 0.01822824962437153\n",
      "Epoch 60, Train Loss: 0.016674017533659935\n",
      "Epoch 70, Train Loss: 0.015018023550510406\n",
      "Epoch 80, Train Loss: 0.013286682777106762\n",
      "Epoch 90, Train Loss: 0.01113374624401331\n",
      "Epoch 100, Train Loss: 0.008656430058181286\n",
      "Epoch 110, Train Loss: 0.00716521218419075\n",
      "Epoch 120, Train Loss: 0.0064131394028663635\n",
      "Epoch 130, Train Loss: 0.005959879141300917\n",
      "Epoch 140, Train Loss: 0.005641344469040632\n",
      "Epoch 150, Train Loss: 0.005377114284783602\n",
      "Epoch 160, Train Loss: 0.005170464515686035\n",
      "Epoch 170, Train Loss: 0.004951815586537123\n",
      "Epoch 180, Train Loss: 0.004777141381055117\n",
      "Epoch 190, Train Loss: 0.004596447106450796\n",
      "Epoch 200, Train Loss: 0.004416464362293482\n",
      "Epoch 210, Train Loss: 0.004221882671117783\n",
      "Epoch 220, Train Loss: 0.004331293981522322\n",
      "Epoch 230, Train Loss: 0.005169689189642668\n",
      "Epoch 240, Train Loss: 0.004166262224316597\n",
      "Epoch 250, Train Loss: 0.003915390931069851\n",
      "Epoch 260, Train Loss: 0.0035878075286746025\n",
      "Epoch 270, Train Loss: 0.003353527979925275\n",
      "Epoch 280, Train Loss: 0.0032008120324462652\n",
      "Epoch 290, Train Loss: 0.01594393327832222\n",
      "Epoch 300, Train Loss: 0.008938251994550228\n",
      "Epoch 310, Train Loss: 0.005510087125003338\n",
      "Epoch 320, Train Loss: 0.004757920745760202\n",
      "Epoch 330, Train Loss: 0.00434696301817894\n",
      "Epoch 340, Train Loss: 0.003969078417867422\n",
      "Epoch 350, Train Loss: 0.0037168224807828665\n",
      "Epoch 360, Train Loss: 0.0035483967512845993\n",
      "Epoch 370, Train Loss: 0.00338958203792572\n",
      "Epoch 380, Train Loss: 0.0032257470302283764\n",
      "Epoch 390, Train Loss: 0.0030582603067159653\n",
      "Epoch 400, Train Loss: 0.002904073800891638\n",
      "Epoch 410, Train Loss: 0.0027799527160823345\n",
      "Epoch 420, Train Loss: 0.002677353098988533\n",
      "Epoch 430, Train Loss: 0.0025876155123114586\n",
      "Epoch 440, Train Loss: 0.00250329845584929\n",
      "Epoch 450, Train Loss: 0.00242254720069468\n",
      "Epoch 460, Train Loss: 0.0023474234621971846\n",
      "Epoch 470, Train Loss: 0.0022785949986428022\n",
      "Epoch 480, Train Loss: 0.002209380967542529\n",
      "Epoch 490, Train Loss: 0.002148300874978304\n",
      "Epoch 500, Train Loss: 0.002094751223921776\n",
      "Epoch 510, Train Loss: 0.0020469336304813623\n",
      "Epoch 520, Train Loss: 0.0020036990754306316\n",
      "Epoch 530, Train Loss: 0.0019640689715743065\n",
      "Epoch 540, Train Loss: 0.0019276622915640473\n",
      "Epoch 550, Train Loss: 0.0018937010318040848\n",
      "Epoch 560, Train Loss: 0.0018613978754729033\n",
      "Epoch 570, Train Loss: 0.0018307896098122\n",
      "Epoch 580, Train Loss: 0.0018016279209405184\n",
      "Epoch 590, Train Loss: 0.0017753529828041792\n",
      "Epoch 600, Train Loss: 0.0017503555864095688\n",
      "Epoch 610, Train Loss: 0.001727288356050849\n",
      "Epoch 620, Train Loss: 0.0017066880827769637\n",
      "Epoch 630, Train Loss: 0.001761727500706911\n",
      "Epoch 640, Train Loss: 0.0018424899317324162\n",
      "Epoch 650, Train Loss: 0.0017896227072924376\n",
      "Epoch 660, Train Loss: 0.001635068329051137\n",
      "Epoch 670, Train Loss: 0.0016168965958058834\n",
      "Epoch 680, Train Loss: 0.0015978739829733968\n",
      "Epoch 690, Train Loss: 0.0015750734601169825\n",
      "Epoch 700, Train Loss: 0.0015575268771499395\n",
      "Epoch 710, Train Loss: 0.0015409865882247686\n",
      "Epoch 720, Train Loss: 0.0015249027637764812\n",
      "Epoch 730, Train Loss: 0.0015093771507963538\n",
      "Epoch 740, Train Loss: 0.0014944208087399602\n",
      "Epoch 750, Train Loss: 0.001479760161601007\n",
      "Epoch 760, Train Loss: 0.00146582443267107\n",
      "Epoch 770, Train Loss: 0.0014526678714901209\n",
      "Epoch 780, Train Loss: 0.0018692519515752792\n",
      "Epoch 790, Train Loss: 0.0016725213499739766\n",
      "Epoch 800, Train Loss: 0.0015591970877721906\n",
      "Epoch 810, Train Loss: 0.0014865469420328736\n",
      "Epoch 820, Train Loss: 0.0014229037333279848\n",
      "Epoch 830, Train Loss: 0.0013950001448392868\n",
      "Epoch 840, Train Loss: 0.001380717265419662\n",
      "Epoch 850, Train Loss: 0.0013673860812559724\n",
      "Epoch 860, Train Loss: 0.0013553762109950185\n",
      "Epoch 870, Train Loss: 0.0013441555202007294\n",
      "Epoch 880, Train Loss: 0.0013331598602235317\n",
      "Epoch 890, Train Loss: 0.0013227423187345266\n",
      "Epoch 900, Train Loss: 0.0013126516714692116\n",
      "Epoch 910, Train Loss: 0.0013028247049078345\n",
      "Epoch 920, Train Loss: 0.001293292618356645\n",
      "Epoch 930, Train Loss: 0.0012840075651183724\n",
      "Epoch 940, Train Loss: 0.0012748721055686474\n",
      "Epoch 950, Train Loss: 0.0012660495704039931\n",
      "Epoch 960, Train Loss: 0.0012573718558996916\n",
      "Epoch 970, Train Loss: 0.0012489630607888103\n",
      "Epoch 980, Train Loss: 0.0012406996684148908\n",
      "Epoch 990, Train Loss: 0.001232601935043931\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model124.train()\n",
    "    \n",
    "    optimizer124.zero_grad()\n",
    "    y_pred = model124(train_seq_tensor4)\n",
    "    \n",
    "    loss = criterion124(y_pred, train_label_tensor4)\n",
    "    loss.backward()\n",
    "    optimizer124.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model124.state_dict, 'model124_30step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 124 dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.009822727435360345\n"
     ]
    }
   ],
   "source": [
    "model124.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_1)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_1[i]\n",
    "    y_test_label = test_labels_1_un[i]\n",
    "    y_pred_test = model124(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * first_piece.std().values + first_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 124 dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.008765140381459976\n"
     ]
    }
   ],
   "source": [
    "model124.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_2)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_2[i]\n",
    "    y_test_label = test_labels_2_un[i]\n",
    "    y_pred_test = model124(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * second_piece.std().values + second_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 124 datset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.007502452232763586\n"
     ]
    }
   ],
   "source": [
    "model124.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_3)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_3[i]\n",
    "    y_test_label = test_labels_3_un[i]\n",
    "    y_pred_test = model124(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * third_piece.std().values + third_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error model 124 dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.0009080997452065353\n"
     ]
    }
   ],
   "source": [
    "model124.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_4)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_4[i]\n",
    "    y_test_label = test_labels_4_un[i]\n",
    "    y_pred_test = model124(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * fourth_piece.std().values + fourth_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
