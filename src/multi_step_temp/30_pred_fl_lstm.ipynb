{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../3OEC_current_flow.csv')\n",
    "\n",
    "df[\"O2_avg\"] = df[[\"O2_S1\", \"O2_S2\", \"O2_S3\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARE_np(prediction, truth):\n",
    "    epsilon = 1e-8  # Small value to prevent division by zero\n",
    "    return np.sum((np.abs(prediction - truth)) / np.abs(truth + epsilon)) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-11 14:00:00\n",
      "2017-07-11 14:00:00 2017-07-12 08:00:00\n",
      "518401\n",
      "2017-07-13 10:59:59.875000\n",
      "2017-07-13 10:59:59.875000 2017-07-14 06:00:00\n",
      "547202\n",
      "2017-07-15 10:00:00\n",
      "2017-07-15 10:00:00 2017-07-16 06:00:00\n",
      "576001\n",
      "2017-07-16 16:00:00\n",
      "2017-07-16 16:00:00 2017-07-17 06:00:00\n",
      "403201\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_time_11 = datetime(2017, 7, 11, 14, 0, 0)\n",
    "end_time_11 = datetime(2017, 7, 12, 8, 0, 0)\n",
    "\n",
    "start_time_13 = datetime(2017, 7, 13, 11, 0, 0)\n",
    "end_time_13 = datetime(2017, 7, 14, 6, 0, 0)\n",
    "\n",
    "start_time_15 = datetime(2017, 7, 15, 10, 0, 0)\n",
    "end_time_15 = datetime(2017, 7, 16, 6, 0, 0)\n",
    "\n",
    "start_time_16 = datetime(2017, 7, 16, 16, 0, 0)\n",
    "end_time_16 = datetime(2017, 7, 17, 6, 0, 0)\n",
    "\n",
    "deployments = {\n",
    "    \"3oec_2017_7_11_12\": {\"start\": start_time_11, \"end\": end_time_11},\n",
    "    \"3oec_2017_7_13_14\": {\"start\": start_time_13, \"end\": end_time_13},\n",
    "    \"3oec_2017_7_15_16\": {\"start\": start_time_15, \"end\": end_time_15},\n",
    "    \"3oec_2017_7_16_17\": {\"start\": start_time_16, \"end\": end_time_16}\n",
    "}\n",
    "\n",
    "date_ranges = []\n",
    "\n",
    "for deployment_name, deployment_info in deployments.items():\n",
    "    start_time = deployment_info[\"start\"]\n",
    "    end_time = deployment_info[\"end\"]\n",
    "    if deployment_name == \"3oec_2017_7_13_14\":\n",
    "        start_time -= timedelta(seconds=0.125)\n",
    "    print(start_time)\n",
    "\n",
    "    # Calculate total seconds and number of measurements\n",
    "    total_seconds = (end_time - start_time).total_seconds() + 0.125\n",
    "    num_measurements = int(total_seconds * 8)\n",
    "\n",
    "    # Create DatetimeIndex for the deployment\n",
    "    date_range = pd.date_range(start=start_time, periods=num_measurements, freq=f'{1000/8}ms')\n",
    "    print(date_range[0], date_range[-1])\n",
    "    print(len(date_range))\n",
    "    date_ranges.append(pd.Series(date_range))\n",
    "\n",
    "# Concatenate all DatetimeIndexes\n",
    "complete_index = pd.concat(date_ranges)\n",
    "\n",
    "# Set the complete index to your DataFrame\n",
    "df.index = complete_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop deployment column and resample\n",
    "df_resampled = df.drop(columns=['deployment', 't', 't_increase', 'Vx', 'Vy', 'Vz', 'P', 'O2_S1', 'O2_S2', 'O2_S3']).resample('1min').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece = df_resampled[\"2017-07-11\":\"2017-07-12 06:00:00\"]\n",
    "second_piece = df_resampled[\"2017-07-13 12:00:00\":\"2017-07-14 06:00:00\"]\n",
    "third_piece = df_resampled[\"2017-07-15 12:00:00\":\"2017-07-16 6:00:00\"]\n",
    "fourth_piece = df_resampled[\"2017-07-16 16:00:00\":\"2017-07-17\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Creates sequences and their corresponding target sequences from the input data.\n",
    "    The target sequence is half the size of the training sequence.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data.\n",
    "        seq_length (int): The length of each training sequence.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of input sequences.\n",
    "        np.ndarray: Array of target sequences (half the length of input sequences).\n",
    "    \"\"\"\n",
    "    target_length = seq_length // 3  # Target is 1/3 the size of the training sequence\n",
    "\n",
    "    if len(data) < seq_length + target_length:\n",
    "        raise ValueError(\"Data length must be at least seq_length + target_length.\")\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - seq_length - target_length + 1):\n",
    "        sequences.append(data[i:i+seq_length])                        # Input sequence\n",
    "        targets.append(data[i+seq_length:i+seq_length+target_length]) # Target sequence\n",
    "\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data piece 1\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(first_piece)\n",
    "train_df1 =first_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean1 = train_df1.mean()\n",
    "train_std1 = train_df1.std()\n",
    "\n",
    "train_df1 = (train_df1 - train_mean1) / train_std1\n",
    "\n",
    "# make sequences\n",
    "train_seq1, train_labels1 = create_sequences(train_df1.values, 90)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor1 = torch.FloatTensor(train_seq1).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor1 = torch.FloatTensor(train_labels1).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 2\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(second_piece)\n",
    "train_df2 =second_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean2 = train_df2.mean()\n",
    "train_std2 = train_df2.std()\n",
    "\n",
    "train_df2 = (train_df2 - train_mean2) / train_std2\n",
    "\n",
    "\n",
    "# make sequences\n",
    "train_seq2, train_labels2 = create_sequences(train_df2.values, 90)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor2 = torch.FloatTensor(train_seq2).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor2 = torch.FloatTensor(train_labels2).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 3\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(third_piece)\n",
    "train_df3 = third_piece\n",
    "\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean3 = train_df3.mean()\n",
    "train_std3 = train_df3.std()\n",
    "\n",
    "train_df3 = (train_df3 - train_mean3) / train_std3\n",
    "\n",
    "# make sequences\n",
    "train_seq3, train_labels3 = create_sequences(train_df3.values, 90)\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor3 = torch.FloatTensor(train_seq3).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor3 = torch.FloatTensor(train_labels3).to(device=device) # (batch, output_dim)\n",
    "\n",
    "# data piece 4\n",
    "column_indices = {name: i for i, name in enumerate(first_piece.columns)}\n",
    "n = len(fourth_piece)\n",
    "train_df4 = fourth_piece\n",
    "\n",
    "# Normalize the data (each partition separately)\n",
    "train_mean4 = train_df4.mean()\n",
    "train_std4 = train_df4.std()\n",
    "\n",
    "train_df4 = (train_df4 - train_mean4) / train_std4\n",
    "\n",
    "# make sequences\n",
    "train_seq4, train_labels4 = create_sequences(train_df4.values, 90)\n",
    "\n",
    "# make tensors\n",
    "train_seq_tensor4 = torch.FloatTensor(train_seq4).to(device=device)  # (batch, seq_length, input_dim)\n",
    "train_label_tensor4 = torch.FloatTensor(train_labels4).to(device=device) # (batch, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test_1 = (first_piece[:] - first_piece.mean())/first_piece.std()\n",
    "test_seq_1, test_labels_1 = create_sequences(normalized_test_1.values, 90)\n",
    "test_tensor_seq_1 = torch.FloatTensor(test_seq_1).to(device=device)\n",
    "test_tensor_labels_1 = torch.FloatTensor(test_labels_1).to(device=device)\n",
    "\n",
    "normalized_test_2 = (second_piece[:] - second_piece.mean())/second_piece.std()\n",
    "test_seq_2, test_labels_2 = create_sequences(normalized_test_2.values, 90)\n",
    "test_tensor_seq_2 = torch.FloatTensor(test_seq_2).to(device=device)\n",
    "test_tensor_labels_2 = torch.FloatTensor(test_labels_2).to(device=device)\n",
    "\n",
    "normalized_test_3 = (third_piece[:] - third_piece.mean())/third_piece.std()\n",
    "test_seq_3, test_labels_3 = create_sequences(normalized_test_3.values, 90)\n",
    "test_tensor_seq_3 = torch.FloatTensor(test_seq_3).to(device='cuda')\n",
    "test_tensor_labels_3 = torch.FloatTensor(test_labels_3).to(device='cuda')\n",
    "\n",
    "normalized_test_4 = (fourth_piece[:] - fourth_piece.mean())/fourth_piece.std()\n",
    "test_seq_4, test_labels_4 = create_sequences(normalized_test_4.values, 90)\n",
    "test_tensor_seq_4 = torch.FloatTensor(test_seq_4).to(device='cuda')\n",
    "test_tensor_labels_4 = torch.FloatTensor(test_labels_4).to(device='cuda')\n",
    "\n",
    "test_labels_1_un = test_labels_1 * first_piece.std().values + first_piece.mean().values\n",
    "test_labels_2_un = test_labels_2 * second_piece.std().values + second_piece.mean().values\n",
    "test_labels_3_un = test_labels_3 * third_piece.std().values + third_piece.mean().values\n",
    "test_labels_4_un = test_labels_4 * fourth_piece.std().values + fourth_piece.mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 30)\n",
    "        # self.fc2 = nn.Linear(30, output_dim)\n",
    "        # self.do = nn.Dropout()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)  # Initial hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)  # Initial cell state\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out = self.do(out)\n",
    "        # print(out.shape)\n",
    "        out = self.fc1(out[:,-1, :]) # take the last hidden state because we assume it encodes all the information about the sequence\n",
    "        # out = self.fc2(out[, :])  # Take the last 30 time step output\n",
    "        return out.unsqueeze(2)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 1\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "output_dim = 1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Instantiate model\n",
    "model1 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model1 = model1.to(\"cuda\")  # Use \"cuda\" if you have a GPU\n",
    "torch.manual_seed(42)\n",
    "model2 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model2 = model2.to(\"cuda\")  # Use \"cuda\" if you have a GPU\n",
    "torch.manual_seed(42)\n",
    "model3 = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "model3 = model3.to(\"cuda\")  # Use \"cuda\" if you have a GPU\n",
    "\n",
    "criterion1 = nn.SmoothL1Loss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.0001)\n",
    "criterion2 = nn.SmoothL1Loss()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.0001)\n",
    "criterion3 = nn.SmoothL1Loss()\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.4783779978752136\n",
      "Epoch 10, Train Loss: 0.4753434956073761\n",
      "Epoch 20, Train Loss: 0.4674391448497772\n",
      "Epoch 30, Train Loss: 0.4318515956401825\n",
      "Epoch 40, Train Loss: 0.2707573175430298\n",
      "Epoch 50, Train Loss: 0.17363795638084412\n",
      "Epoch 60, Train Loss: 0.130163311958313\n",
      "Epoch 70, Train Loss: 0.1158125102519989\n",
      "Epoch 80, Train Loss: 0.09714887291193008\n",
      "Epoch 90, Train Loss: 0.08358844369649887\n",
      "Epoch 100, Train Loss: 0.07416834682226181\n",
      "Epoch 110, Train Loss: 0.06857936829328537\n",
      "Epoch 120, Train Loss: 0.0651521161198616\n",
      "Epoch 130, Train Loss: 0.062423255294561386\n",
      "Epoch 140, Train Loss: 0.05998631939291954\n",
      "Epoch 150, Train Loss: 0.05767514929175377\n",
      "Epoch 160, Train Loss: 0.055317994207143784\n",
      "Epoch 170, Train Loss: 0.052738599479198456\n",
      "Epoch 180, Train Loss: 0.049704402685165405\n",
      "Epoch 190, Train Loss: 0.04572592303156853\n",
      "Epoch 200, Train Loss: 0.039890389889478683\n",
      "Epoch 210, Train Loss: 0.036315787583589554\n",
      "Epoch 220, Train Loss: 0.03492087870836258\n",
      "Epoch 230, Train Loss: 0.02969943918287754\n",
      "Epoch 240, Train Loss: 0.026784928515553474\n",
      "Epoch 250, Train Loss: 0.024526286870241165\n",
      "Epoch 260, Train Loss: 0.022992953658103943\n",
      "Epoch 270, Train Loss: 0.019747387617826462\n",
      "Epoch 280, Train Loss: 0.017957154661417007\n",
      "Epoch 290, Train Loss: 0.016643591225147247\n",
      "Epoch 300, Train Loss: 0.015593336895108223\n",
      "Epoch 310, Train Loss: 0.014882644638419151\n",
      "Epoch 320, Train Loss: 0.014312438666820526\n",
      "Epoch 330, Train Loss: 0.013816332444548607\n",
      "Epoch 340, Train Loss: 0.013361386954784393\n",
      "Epoch 350, Train Loss: 0.012934005819261074\n",
      "Epoch 360, Train Loss: 0.0125347301363945\n",
      "Epoch 370, Train Loss: 0.012154767289757729\n",
      "Epoch 380, Train Loss: 0.011969414539635181\n",
      "Epoch 390, Train Loss: 0.011545070447027683\n",
      "Epoch 400, Train Loss: 0.011164250783622265\n",
      "Epoch 410, Train Loss: 0.010768026113510132\n",
      "Epoch 420, Train Loss: 0.010347940027713776\n",
      "Epoch 430, Train Loss: 0.009980314411222935\n",
      "Epoch 440, Train Loss: 0.009431508369743824\n",
      "Epoch 450, Train Loss: 0.008932465687394142\n",
      "Epoch 460, Train Loss: 0.00853019580245018\n",
      "Epoch 470, Train Loss: 0.008222104050219059\n",
      "Epoch 480, Train Loss: 0.007880165241658688\n",
      "Epoch 490, Train Loss: 0.007646093610674143\n",
      "Epoch 500, Train Loss: 0.007418747525662184\n",
      "Epoch 510, Train Loss: 0.0074356840923428535\n",
      "Epoch 520, Train Loss: 0.007795076817274094\n",
      "Epoch 530, Train Loss: 0.010623177513480186\n",
      "Epoch 540, Train Loss: 0.007724204566329718\n",
      "Epoch 550, Train Loss: 0.007088988553732634\n",
      "Epoch 560, Train Loss: 0.006764309946447611\n",
      "Epoch 570, Train Loss: 0.006562040187418461\n",
      "Epoch 580, Train Loss: 0.0064375693909823895\n",
      "Epoch 590, Train Loss: 0.006324055138975382\n",
      "Epoch 600, Train Loss: 0.006351214833557606\n",
      "Epoch 610, Train Loss: 0.006220993585884571\n",
      "Epoch 620, Train Loss: 0.006111344322562218\n",
      "Epoch 630, Train Loss: 0.006210844032466412\n",
      "Epoch 640, Train Loss: 0.006311565637588501\n",
      "Epoch 650, Train Loss: 0.0061403801664710045\n",
      "Epoch 660, Train Loss: 0.005878552794456482\n",
      "Epoch 670, Train Loss: 0.005823626182973385\n",
      "Epoch 680, Train Loss: 0.005829117726534605\n",
      "Epoch 690, Train Loss: 0.0059832180850207806\n",
      "Epoch 700, Train Loss: 0.00571802444756031\n",
      "Epoch 710, Train Loss: 0.00565940048545599\n",
      "Epoch 720, Train Loss: 0.0056950123980641365\n",
      "Epoch 730, Train Loss: 0.00580422580242157\n",
      "Epoch 740, Train Loss: 0.006324537098407745\n",
      "Epoch 750, Train Loss: 0.006326666101813316\n",
      "Epoch 760, Train Loss: 0.006076341960579157\n",
      "Epoch 770, Train Loss: 0.005680687725543976\n",
      "Epoch 780, Train Loss: 0.005560123361647129\n",
      "Epoch 790, Train Loss: 0.005431220401078463\n",
      "Epoch 800, Train Loss: 0.0053665353916585445\n",
      "Epoch 810, Train Loss: 0.005317005328834057\n",
      "Epoch 820, Train Loss: 0.005297486670315266\n",
      "Epoch 830, Train Loss: 0.0052540358155965805\n",
      "Epoch 840, Train Loss: 0.005194123834371567\n",
      "Epoch 850, Train Loss: 0.005342131480574608\n",
      "Epoch 860, Train Loss: 0.006132772658020258\n",
      "Epoch 870, Train Loss: 0.005826099775731564\n",
      "Epoch 880, Train Loss: 0.005280733574181795\n",
      "Epoch 890, Train Loss: 0.00525938905775547\n",
      "Epoch 900, Train Loss: 0.005207411013543606\n",
      "Epoch 910, Train Loss: 0.005012519657611847\n",
      "Epoch 920, Train Loss: 0.005303349811583757\n",
      "Epoch 930, Train Loss: 0.005010610446333885\n",
      "Epoch 940, Train Loss: 0.005384646821767092\n",
      "Epoch 950, Train Loss: 0.0046530854888260365\n",
      "Epoch 960, Train Loss: 0.004461074247956276\n",
      "Epoch 970, Train Loss: 0.0043444158509373665\n",
      "Epoch 980, Train Loss: 0.004230393096804619\n",
      "Epoch 990, Train Loss: 0.0042004031129181385\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    \n",
    "    optimizer1.zero_grad()\n",
    "    y_pred = model1(train_seq_tensor1)\n",
    "    \n",
    "    loss = criterion1(y_pred, train_label_tensor1)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.47595566511154175\n",
      "Epoch 10, Train Loss: 0.47315648198127747\n",
      "Epoch 20, Train Loss: 0.46556615829467773\n",
      "Epoch 30, Train Loss: 0.4320449233055115\n",
      "Epoch 40, Train Loss: 0.25820955634117126\n",
      "Epoch 50, Train Loss: 0.13331331312656403\n",
      "Epoch 60, Train Loss: 0.10100923478603363\n",
      "Epoch 70, Train Loss: 0.08269467949867249\n",
      "Epoch 80, Train Loss: 0.06095331907272339\n",
      "Epoch 90, Train Loss: 0.04979470372200012\n",
      "Epoch 100, Train Loss: 0.04579707235097885\n",
      "Epoch 110, Train Loss: 0.04446949437260628\n",
      "Epoch 120, Train Loss: 0.043195661157369614\n",
      "Epoch 130, Train Loss: 0.04217256233096123\n",
      "Epoch 140, Train Loss: 0.04134571552276611\n",
      "Epoch 150, Train Loss: 0.040572602301836014\n",
      "Epoch 160, Train Loss: 0.039817336946725845\n",
      "Epoch 170, Train Loss: 0.03906415030360222\n",
      "Epoch 180, Train Loss: 0.03829026594758034\n",
      "Epoch 190, Train Loss: 0.03747615963220596\n",
      "Epoch 200, Train Loss: 0.036587804555892944\n",
      "Epoch 210, Train Loss: 0.035570502281188965\n",
      "Epoch 220, Train Loss: 0.03426782786846161\n",
      "Epoch 230, Train Loss: 0.03210289403796196\n",
      "Epoch 240, Train Loss: 0.027615373954176903\n",
      "Epoch 250, Train Loss: 0.024819673970341682\n",
      "Epoch 260, Train Loss: 0.02376331202685833\n",
      "Epoch 270, Train Loss: 0.02288665808737278\n",
      "Epoch 280, Train Loss: 0.02204401046037674\n",
      "Epoch 290, Train Loss: 0.021876133978366852\n",
      "Epoch 300, Train Loss: 0.02122914046049118\n",
      "Epoch 310, Train Loss: 0.02063041366636753\n",
      "Epoch 320, Train Loss: 0.020455118268728256\n",
      "Epoch 330, Train Loss: 0.020209912210702896\n",
      "Epoch 340, Train Loss: 0.019541854038834572\n",
      "Epoch 350, Train Loss: 0.01919659972190857\n",
      "Epoch 360, Train Loss: 0.019408883526921272\n",
      "Epoch 370, Train Loss: 0.018796537071466446\n",
      "Epoch 380, Train Loss: 0.019000858068466187\n",
      "Epoch 390, Train Loss: 0.01825796440243721\n",
      "Epoch 400, Train Loss: 0.017933642491698265\n",
      "Epoch 410, Train Loss: 0.017641758546233177\n",
      "Epoch 420, Train Loss: 0.017410390079021454\n",
      "Epoch 430, Train Loss: 0.017232408747076988\n",
      "Epoch 440, Train Loss: 0.018092123791575432\n",
      "Epoch 450, Train Loss: 0.01766945980489254\n",
      "Epoch 460, Train Loss: 0.017110588029026985\n",
      "Epoch 470, Train Loss: 0.01696017198264599\n",
      "Epoch 480, Train Loss: 0.016803711652755737\n",
      "Epoch 490, Train Loss: 0.016693131998181343\n",
      "Epoch 500, Train Loss: 0.01660364307463169\n",
      "Epoch 510, Train Loss: 0.016509104520082474\n",
      "Epoch 520, Train Loss: 0.016552018001675606\n",
      "Epoch 530, Train Loss: 0.01646142639219761\n",
      "Epoch 540, Train Loss: 0.016282644122838974\n",
      "Epoch 550, Train Loss: 0.0161899384111166\n",
      "Epoch 560, Train Loss: 0.01608709990978241\n",
      "Epoch 570, Train Loss: 0.016262313351035118\n",
      "Epoch 580, Train Loss: 0.01593519002199173\n",
      "Epoch 590, Train Loss: 0.015840625390410423\n",
      "Epoch 600, Train Loss: 0.015712793916463852\n",
      "Epoch 610, Train Loss: 0.015616661868989468\n",
      "Epoch 620, Train Loss: 0.01549118384718895\n",
      "Epoch 630, Train Loss: 0.015420100651681423\n",
      "Epoch 640, Train Loss: 0.015293420292437077\n",
      "Epoch 650, Train Loss: 0.015096218325197697\n",
      "Epoch 660, Train Loss: 0.014961385168135166\n",
      "Epoch 670, Train Loss: 0.014957730658352375\n",
      "Epoch 680, Train Loss: 0.014881298877298832\n",
      "Epoch 690, Train Loss: 0.014612558297812939\n",
      "Epoch 700, Train Loss: 0.014437278732657433\n",
      "Epoch 710, Train Loss: 0.014271426014602184\n",
      "Epoch 720, Train Loss: 0.014149629510939121\n",
      "Epoch 730, Train Loss: 0.02090795524418354\n",
      "Epoch 740, Train Loss: 0.017756715416908264\n",
      "Epoch 750, Train Loss: 0.015448805876076221\n",
      "Epoch 760, Train Loss: 0.014776567928493023\n",
      "Epoch 770, Train Loss: 0.014688912779092789\n",
      "Epoch 780, Train Loss: 0.01455587800592184\n",
      "Epoch 790, Train Loss: 0.014431695453822613\n",
      "Epoch 800, Train Loss: 0.014357801526784897\n",
      "Epoch 810, Train Loss: 0.014286070130765438\n",
      "Epoch 820, Train Loss: 0.01422118954360485\n",
      "Epoch 830, Train Loss: 0.014162342995405197\n",
      "Epoch 840, Train Loss: 0.014109158888459206\n",
      "Epoch 850, Train Loss: 0.01409678440541029\n",
      "Epoch 860, Train Loss: 0.014091122895479202\n",
      "Epoch 870, Train Loss: 0.014010979793965816\n",
      "Epoch 880, Train Loss: 0.013925381936132908\n",
      "Epoch 890, Train Loss: 0.013840421102941036\n",
      "Epoch 900, Train Loss: 0.01377145666629076\n",
      "Epoch 910, Train Loss: 0.013698822818696499\n",
      "Epoch 920, Train Loss: 0.01361080165952444\n",
      "Epoch 930, Train Loss: 0.013513314537703991\n",
      "Epoch 940, Train Loss: 0.013412069529294968\n",
      "Epoch 950, Train Loss: 0.013301442377269268\n",
      "Epoch 960, Train Loss: 0.013182984665036201\n",
      "Epoch 970, Train Loss: 0.013051546178758144\n",
      "Epoch 980, Train Loss: 0.012981886975467205\n",
      "Epoch 990, Train Loss: 0.012953017838299274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    y_pred = model2(train_seq_tensor2)\n",
    "    \n",
    "    loss = criterion2(y_pred, train_label_tensor2)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.4691829979419708\n",
      "Epoch 10, Train Loss: 0.4662507176399231\n",
      "Epoch 20, Train Loss: 0.4586041271686554\n",
      "Epoch 30, Train Loss: 0.4250997006893158\n",
      "Epoch 40, Train Loss: 0.24706730246543884\n",
      "Epoch 50, Train Loss: 0.15700815618038177\n",
      "Epoch 60, Train Loss: 0.12598277628421783\n",
      "Epoch 70, Train Loss: 0.10352298617362976\n",
      "Epoch 80, Train Loss: 0.09350504726171494\n",
      "Epoch 90, Train Loss: 0.08614220470190048\n",
      "Epoch 100, Train Loss: 0.08111482858657837\n",
      "Epoch 110, Train Loss: 0.07696013152599335\n",
      "Epoch 120, Train Loss: 0.073519267141819\n",
      "Epoch 130, Train Loss: 0.0704818144440651\n",
      "Epoch 140, Train Loss: 0.0677594542503357\n",
      "Epoch 150, Train Loss: 0.0652124285697937\n",
      "Epoch 160, Train Loss: 0.06262194365262985\n",
      "Epoch 170, Train Loss: 0.05979756638407707\n",
      "Epoch 180, Train Loss: 0.05661069601774216\n",
      "Epoch 190, Train Loss: 0.053581736981868744\n",
      "Epoch 200, Train Loss: 0.050707995891571045\n",
      "Epoch 210, Train Loss: 0.04723330959677696\n",
      "Epoch 220, Train Loss: 0.04386843368411064\n",
      "Epoch 230, Train Loss: 0.04125727340579033\n",
      "Epoch 240, Train Loss: 0.03960193693637848\n",
      "Epoch 250, Train Loss: 0.03828544169664383\n",
      "Epoch 260, Train Loss: 0.037186749279499054\n",
      "Epoch 270, Train Loss: 0.035645730793476105\n",
      "Epoch 280, Train Loss: 0.03538249060511589\n",
      "Epoch 290, Train Loss: 0.03177659586071968\n",
      "Epoch 300, Train Loss: 0.03009863756597042\n",
      "Epoch 310, Train Loss: 0.028963826596736908\n",
      "Epoch 320, Train Loss: 0.028136542066931725\n",
      "Epoch 330, Train Loss: 0.027617283165454865\n",
      "Epoch 340, Train Loss: 0.027109861373901367\n",
      "Epoch 350, Train Loss: 0.027184152975678444\n",
      "Epoch 360, Train Loss: 0.026831351220607758\n",
      "Epoch 370, Train Loss: 0.02624032273888588\n",
      "Epoch 380, Train Loss: 0.025609835982322693\n",
      "Epoch 390, Train Loss: 0.02524568885564804\n",
      "Epoch 400, Train Loss: 0.024727733805775642\n",
      "Epoch 410, Train Loss: 0.024288177490234375\n",
      "Epoch 420, Train Loss: 0.023922936990857124\n",
      "Epoch 430, Train Loss: 0.023600909858942032\n",
      "Epoch 440, Train Loss: 0.023301808163523674\n",
      "Epoch 450, Train Loss: 0.022992698475718498\n",
      "Epoch 460, Train Loss: 0.022635526955127716\n",
      "Epoch 470, Train Loss: 0.02214362844824791\n",
      "Epoch 480, Train Loss: 0.02139914594590664\n",
      "Epoch 490, Train Loss: 0.02047409676015377\n",
      "Epoch 500, Train Loss: 0.02135973796248436\n",
      "Epoch 510, Train Loss: 0.022943709045648575\n",
      "Epoch 520, Train Loss: 0.02089763805270195\n",
      "Epoch 530, Train Loss: 0.019723491743206978\n",
      "Epoch 540, Train Loss: 0.018689684569835663\n",
      "Epoch 550, Train Loss: 0.01826534979045391\n",
      "Epoch 560, Train Loss: 0.01795283704996109\n",
      "Epoch 570, Train Loss: 0.017708007246255875\n",
      "Epoch 580, Train Loss: 0.017499269917607307\n",
      "Epoch 590, Train Loss: 0.017302753403782845\n",
      "Epoch 600, Train Loss: 0.01711755059659481\n",
      "Epoch 610, Train Loss: 0.016938354820013046\n",
      "Epoch 620, Train Loss: 0.016762761399149895\n",
      "Epoch 630, Train Loss: 0.016587959602475166\n",
      "Epoch 640, Train Loss: 0.016411129385232925\n",
      "Epoch 650, Train Loss: 0.016233082860708237\n",
      "Epoch 660, Train Loss: 0.016046565026044846\n",
      "Epoch 670, Train Loss: 0.015855032950639725\n",
      "Epoch 680, Train Loss: 0.015653075650334358\n",
      "Epoch 690, Train Loss: 0.015443981625139713\n",
      "Epoch 700, Train Loss: 0.015217640437185764\n",
      "Epoch 710, Train Loss: 0.015172449871897697\n",
      "Epoch 720, Train Loss: 0.014739473350346088\n",
      "Epoch 730, Train Loss: 0.01419916469603777\n",
      "Epoch 740, Train Loss: 0.019883107393980026\n",
      "Epoch 750, Train Loss: 0.01886276714503765\n",
      "Epoch 760, Train Loss: 0.016295162960886955\n",
      "Epoch 770, Train Loss: 0.015096601098775864\n",
      "Epoch 780, Train Loss: 0.01428684126585722\n",
      "Epoch 790, Train Loss: 0.013186560943722725\n",
      "Epoch 800, Train Loss: 0.012370464392006397\n",
      "Epoch 810, Train Loss: 0.012736509554088116\n",
      "Epoch 820, Train Loss: 0.015898622572422028\n",
      "Epoch 830, Train Loss: 0.012166768312454224\n",
      "Epoch 840, Train Loss: 0.011517191305756569\n",
      "Epoch 850, Train Loss: 0.010831817984580994\n",
      "Epoch 860, Train Loss: 0.010224428959190845\n",
      "Epoch 870, Train Loss: 0.010372350923717022\n",
      "Epoch 880, Train Loss: 0.010306194424629211\n",
      "Epoch 890, Train Loss: 0.01306718960404396\n",
      "Epoch 900, Train Loss: 0.013479623943567276\n",
      "Epoch 910, Train Loss: 0.011070672422647476\n",
      "Epoch 920, Train Loss: 0.010411638766527176\n",
      "Epoch 930, Train Loss: 0.009656926617026329\n",
      "Epoch 940, Train Loss: 0.009259790182113647\n",
      "Epoch 950, Train Loss: 0.008969242684543133\n",
      "Epoch 960, Train Loss: 0.008741502650082111\n",
      "Epoch 970, Train Loss: 0.008563119918107986\n",
      "Epoch 980, Train Loss: 0.008413899689912796\n",
      "Epoch 990, Train Loss: 0.008287632837891579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model3.train()\n",
    "    \n",
    "    optimizer3.zero_grad()\n",
    "    y_pred = model3(train_seq_tensor3)\n",
    "    \n",
    "    loss = criterion3(y_pred, train_label_tensor3)\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), 'fl1_30step')\n",
    "torch.save(model2.state_dict(), 'fl2_30step')\n",
    "torch.save(model3.state_dict(), 'fl3_30step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd1 = model1.state_dict()\n",
    "sd2 = model2.state_dict()\n",
    "sd3 = model3.state_dict()\n",
    "avg_model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n",
    "avg_model = avg_model.to(device='cuda')\n",
    "sd_avg = avg_model.state_dict()\n",
    "for key in sd1:\n",
    "    sd_avg[key] = (sd1[key] + sd2[key] + sd3[key])/3\n",
    "\n",
    "avg_model.load_state_dict(sd_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1 = sd1[\"lstm.weight_hh_l0\"].view(-1).cpu().numpy()  # flatten\n",
    "w2 = sd2[\"lstm.weight_hh_l0\"].view(-1).cpu().numpy()  # flatten\n",
    "w3 = sd3[\"lstm.weight_hh_l0\"].view(-1).cpu().numpy() #flatten\n",
    "w4 = sd_avg[\"lstm.weight_hh_l0\"].view(-1).cpu().numpy() #flatten\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(w1, bins=50, alpha=0.5, label='Model 1')\n",
    "plt.hist(w2, bins=50, alpha=0.5, label='Model 2')\n",
    "# plt.hist(w3, bins=50, alpha=0.5, label='Model 3')\n",
    "# plt.hist(w4, bins=50, alpha=0.5, label='Model AVG')\n",
    "plt.legend()\n",
    "plt.title(\"lstm.weight_hh_l0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error FL123 dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.006037243409262737\n"
     ]
    }
   ],
   "source": [
    "avg_model.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_1)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_1[i]\n",
    "    y_test_label = test_labels_1_un[i]\n",
    "    y_pred_test = avg_model(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * first_piece.std().values + first_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error FL123 dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.005886397640767827\n"
     ]
    }
   ],
   "source": [
    "avg_model.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_2)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_2[i]\n",
    "    y_test_label = test_labels_2_un[i]\n",
    "    y_pred_test = avg_model(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * second_piece.std().values + second_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error FL123 dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.004887510934582061\n"
     ]
    }
   ],
   "source": [
    "avg_model.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_3)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_3[i]\n",
    "    y_test_label = test_labels_3_un[i]\n",
    "    y_pred_test = avg_model(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * third_piece.std().values + third_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg error FL 123 dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG error across sequences: 0.006128550056547317\n"
     ]
    }
   ],
   "source": [
    "avg_model.eval()\n",
    "error = 0\n",
    "n_sequences = len(test_tensor_seq_4)\n",
    "for i in range(n_sequences):\n",
    "    # ignore the last step since we don't have real data to compare with\n",
    "    y_test_seq = test_tensor_seq_4[i]\n",
    "    y_test_label = test_labels_4_un[i]\n",
    "    y_pred_test = avg_model(y_test_seq.unsqueeze(0))\n",
    "    # un-normalize prediction and label\n",
    "    y_pred_test = y_pred_test.squeeze(0).detach().cpu().numpy() * fourth_piece.std().values + fourth_piece.mean().values\n",
    "    mare = MARE_np(y_pred_test, y_test_label).item()\n",
    "    error += mare\n",
    "\n",
    "avg_error = error/n_sequences\n",
    "print(f'AVG error across sequences: {avg_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
